## 22. Appendix: Mathematics for Deep Learning

Brent Werness (Amazon), Rachel Hu (Amazon), and authors of this book

One of the wonderful parts of modern deep learning is the fact that much of it can be understood and used without a full understanding of the mathematics below it. This is a sign that the field is maturing. Just as most software developers no longer need to worry about the theory of computable functions, neither should deep learning practitioners need to worry about the theoretical foundations of maximum likelihood learning.

Wonderful : bagus / sangat bagus 
that : itu / bahwa
of : dari
of it can be understood : dari itu dapat dipahami
used : digunakan
without : tanpa
understand : memahami
understanding : pemahaman
matur : dewasa / menantan
field : lapangan / bidang
as : seperti
most : paling / terbanyak
longer : lebih lama
need : perlu / membutuhkan
worry : khawatir
computable : dapat dihitung
neither : juga tidak
should : sebaiknya
neither should : tidak seharusnya
likelihood : kemungkinan


Salah satu bagian yang menajubkan dari deep learning modern adalah fakta bahwa banyak dari itu dapat dipahami dan digunakan tanpa pemahaman yang lengkap tentang matematika dibawahnya. Hal ini menunjukkan bahwa bidang ini sedang tumbuh dewasa. Seperti kebanyakan pengembang perangkat lunak tidak perlu lagi khawatir tentang teori fungsi yang dapat dihitung, para praktisi deep learning juga tidak perlu khawatir tentang dasar teoretis dari pembelajaran likelihood maksimum.

- Likelihood maksimum (maximum likelihood) adalah sebuah metode untuk mencari nilai parameter yang paling mungkin menjelaskan data yang diamati. Dalam metode ini, kita mengasumsikan bahwa data yang diamati berasal dari suatu distribusi probabilistik yang dipengaruhi oleh beberapa parameter. Tujuan dari maximum likelihood adalah menemukan nilai-nilai parameter yang paling mungkin menghasilkan data yang diamati.

- Contoh sederhana penggunaan metode likelihood maksimum adalah dalam estimasi probabilitas suatu koin yang melempar kepala. Jika kita mengamati bahwa dari 10 kali lemparan, kepala muncul sebanyak 7 kali, kita dapat menggunakan metode likelihood maksimum untuk memperkirakan probabilitas koin yang melempar kepala. Dalam kasus ini, kita akan mencari nilai parameter yang membuat likelihood dari data 7 kepala dan 3 ekor paling besar, yang kemudian memberikan perkiraan probabilitas koin yang melempar kepala.

- Probabilitas adalah ukuran numerik dari kemungkinan terjadinya suatu kejadian atau peristiwa. Probabilitas mengukur seberapa besar kemungkinan suatu kejadian atau peristiwa akan terjadi. Probabilitas dinyatakan dalam angka antara 0 dan 1, di mana 0 berarti kejadian itu tidak mungkin terjadi dan 1 berarti kejadian itu pasti terjadi.

- Dalam matematika dan statistika, probabilitas digunakan untuk memprediksi hasil acak. Misalnya, jika kita melempar sebuah koin, probabilitas kepala atau ekor muncul adalah 0,5 atau 50%, asumsi kita tidak ada kecurangan pada koin tersebut. Demikian pula, dalam perjudian, probabilitas menentukan seberapa besar peluang seseorang menang atau kalah dalam suatu permainan.

- Probabilitas dapat dihitung menggunakan beberapa metode, termasuk eksperimen, simulasi, dan perhitungan matematis yang kompleks. Probabilitas sering digunakan dalam banyak bidang, seperti ilmu pengetahuan, teknik, keuangan, dan bisnis, dan merupakan salah satu konsep kunci dalam statistika dan teori peluang.


But, we are not quite there yet.

In practice, you will sometimes need to understand how architectural choices influence gradient flow, or the implicit assumptions you make by training with a certain loss function. You might need to know what in the world entropy measures, and how it can help you understand exactly what bits-per-character means in your model. These all require deeper mathematical understanding.


*there : disana
yet : belum
quite : luamayan / cukup
sometimes : kadang-kadang
choices : pilihan
influence : pengaruh
flow : aliran
implicit : tersirat / mutlak
assumptions : assumptions
make : membuat / melakukan
certain : yakin / tertentu
loss : kehilangan
might : mungkin / semoga
measures : pengukuran
it : dia
exactly : tepat / persis
means : cara / upaya
these : ini
require : membutuhkan
deeper : lebih dalam*

tetapi kita belum cukup sampai disana.

dalam peraktek, terkadang kamu perlu memahami bagaimana pemilihan arsitektur dapat mempengaruhi aliran gradient, atau asumsi implisit yang dibuat saat melatih dengan fungsi loss tertentu. kamu mungkin perlu tau apa itu ukuran entropi dunia, dan bagimana itu dapat membantu kamu memahami dengan tepat apa yang dimaksud dengan bit per karakter dalam model kita. ini semua membutuhkan pemahaman matematika yang lebih dalam.

- Aliran gradient (gradient flow) adalah sebuah konsep matematika yang sering digunakan dalam bidang optimisasi dan analisis numerik. Aliran gradient adalah suatu proses di mana kita menggerakkan suatu sistem pada arah yang menurunkan fungsi objektif.

- Secara khusus, aliran gradient adalah metode numerik yang digunakan untuk menemukan nilai minimum (atau maksimum) suatu fungsi dengan menentukan arah di mana kita harus bergerak dari titik awal yang diberikan untuk mencapai nilai minimum tersebut. Arah ini ditentukan oleh vektor gradient, yang merupakan vektor yang menunjukkan arah dengan penurunan terbesar dalam fungsi.

- Proses aliran gradient dapat digunakan untuk menyelesaikan berbagai masalah, seperti masalah optimisasi, persamaan diferensial parsial, dan model matematika lainnya. Dalam praktiknya, aliran gradient sering digunakan dalam machine learning, khususnya dalam teknik-teknik seperti jaringan syaraf tiruan (neural networks), karena algoritma pembelajaran mesin sering memerlukan pencarian nilai minimum atau maksimum dari fungsi objektif.

- Dalam prakteknya, aliran gradient sering digunakan untuk menemukan nilai minimum atau maksimum pada fungsi dengan banyak variabel atau bahkan ribuan variabel, seperti yang sering terjadi dalam pembelajaran mesin (machine learning).

- Fungsi loss atau fungsi kerugian adalah suatu fungsi matematika yang digunakan untuk mengukur seberapa baik model machine learning mampu memprediksi nilai target berdasarkan data yang diberikan. Fungsi ini sering juga disebut dengan fungsi objektif (objective function) atau fungsi biaya (cost function).

- Tujuan dari fungsi loss adalah untuk menentukan seberapa jauh prediksi model dari nilai target yang sebenarnya. Semakin kecil nilai fungsi loss, maka semakin akurat model dalam memprediksi nilai target.

- Contoh fungsi loss yang sering digunakan adalah mean squared error (MSE) untuk regresi dan cross-entropy loss untuk klasifikasi. Mean squared error mengukur rata-rata selisih kuadrat antara nilai prediksi dan nilai target, sedangkan cross-entropy loss mengukur perbedaan antara distribusi probabilitas prediksi dengan distribusi probabilitas yang sebenarnya.

- Fungsi loss yang dipilih tergantung pada jenis model dan jenis masalah yang akan dipecahkan. Selain itu, dalam beberapa kasus, fungsi loss dapat disesuaikan dengan kebutuhan pengguna atau masalah yang sedang dihadapi.

This appendix aims to provide you the mathematical background you need to understand the core theory of modern deep learning, but it is not exhaustive. We will begin with examining linear algebra in greater depth. We develop a geometric understanding of all the common linear algebraic objects and operations that will enable us to visualize the effects of various transformations on our data. A key element is the development of the basics of eigen-decompositions.

appendix : lampiran
aims : tujuan
provide : menyediakan
exhaustive : lengkap
begin : mulai
examining m: memriksa
greater : lebih besar
depth : mendalam
common : umum
enable : memungkinkan
us : kita
various : bermacam - macam
our : kita
core : inti


lampiran ini bertujuan untuk memberikan latar belakang matematika yang diperlukan untuk memahami teori inti dari deep learning modern, namun tidak seluruhnya mencakup semuanya. Kita akan memulai dengan mengkaji algebra linier dengan lebih dalam. Kita akan mengembangkan pemahaman geometri dari semua objek dan operasi algebra linier yang umum sehingga dapat memvisualisasikan efek dari berbagai transformasi pada data kita. Elemen penting yang dikembangkan adalah dasar-dasar dekomposisi eigen.


We next develop the theory of differential calculus to the point that we can fully understand why the gradient is the direction of steepest descent, and why back-propagation takes the form it does. Integral calculus is then discussed to the degree needed to support our next topic, probability theory.

the point : intinya 
direction : arah
steepsh : curam
descent : turun
take : mengambil
then : kemudian
discussed : dibahas
degree : derjat / tingkat


kita selanjutnya mengembangkan teori kalkulus diferensial hingga kita dapat sepenuhnya mengerti mengapa gradient adalah arah yang paling curam dan mengapa back-propagation memiliki bentuk seperti itu. Kalkulus integral kemudian dibahas sesuai dengan yang dibutuhkan untuk mendukung topik selanjutnya, teori probabilitas.


Problems encountered in practice frequently are not certain, and thus we need a language to speak about uncertain things. We review the theory of random variables and the most commonly encountered distributions so we may discuss models probabilistically. This provides the foundation for the naive Bayes classifier, a probabilistic classification technique.

encountered : dihadapi / mengalami
frequently : sering
thus : jadi / maka
uncertain : tidak pasti
things : sesuatu
commonly : umumnya
may : boleh
foundation : dasar

Masalah yang dihadapi didalam praktek sering tidak pasti, jadi kita memerlukan bahasa untuk berbicara tentang sesuatu yang tidak pasti. kami meninjau teori variabel acak dan distribusi paling umum dihadapi sehingga kita bisa mendiskusikan model probabilitas. Ini menyediakan dasar untuk clasifikasi naive bayes, teknik klasifikasi probabilistik.

Closely related to probability theory is the study of statistics. While statistics is far too large a field to do justice in a short section, we will introduce fundamental concepts that all machine learning practitioners should be aware of, in particular: evaluating and comparing estimators, conducting hypothesis tests, and constructing confidence intervals.

closely : rapat / erat
related : terkait
while : ketika
far : jauh
too : juga 
large : besar
field : bidang 
justice : keadilan
short : pendek
section : bagian
introduce : memperkenalkan
should : sebaiknya / sebenarnya
aware : menyadari
particular : khusus / tertentu
conducting : melakukan
contructing : membangun
confidence : kepercayaan
intervals : selang

terkait erat teori probabilitas dengan studi statistik. ketika ilmu statistik adalah bidang yang sangat luas dan tidak dapat diterapkan dalam bagian pendek. kita akan memperkanalkan konsep dasar yang harus diketahui oleh semua praktisi machine learning, khususnya mengevaluasi dan membandingkan estimatir, melakukan uji hipotesism dan membangun interval keyakinan.

Last, we turn to the topic of information theory, which is the mathematical study of information storage and transmission. This provides the core language by which we may discuss quantitatively how much information a model holds on a domain of discourse.

last : terkahir 
turn : beralih / berbelok / berubah
of : dari / tentang / untuk
hold : memegang / menahan / memiliki

terakhir, kita beralih kedalam topik teori informasi, yang merupakana belajar matematika tentang penyimpanan informasi dan tranmission informasi. Ini menyediakan inti bahasa yang mana kita gunakan untuk mendiskusikan secara kuantitatif berapa banyak informasi yang disimpan oleh suatu model dalam domain diskursus tertentu.

Perbedaan kualitatif dan kuantitatif adalah sebagai berikut:

1. Pendekatan: Pendekatan kualitatif berfokus pada deskripsi dan interpretasi data yang didapat dari pengamatan, wawancara, atau observasi, sedangkan pendekatan kuantitatif berfokus pada pengukuran dan analisis data yang diperoleh melalui angka dan statistik.

2. Data: Pendekatan kualitatif menggunakan data non-numerik, seperti teks, gambar, atau suara, sedangkan pendekatan kuantitatif menggunakan data numerik, seperti angka, persentase, atau rasio.

3. Tujuan: Tujuan pendekatan kualitatif adalah untuk memahami fenomena secara menyeluruh dan mendalam, sedangkan tujuan pendekatan kuantitatif adalah untuk menguji hipotesis dan membuat generalisasi tentang populasi.

4. Subjektivitas: Pendekatan kualitatif lebih cenderung subjektif karena data yang diperoleh dapat dipengaruhi oleh sudut pandang dan interpretasi peneliti, sedangkan pendekatan kuantitatif lebih objektif karena pengambilan data dilakukan secara sistematis dan terukur.

5. Pemilihan sampel: Pendekatan kualitatif menggunakan sampel yang lebih kecil dan tidak mewakili seluruh populasi, sedangkan pendekatan kuantitatif menggunakan sampel yang besar dan representatif untuk dapat membuat generalisasi tentang populasi.

6. Analisis data: Pendekatan kualitatif menggunakan analisis tematik atau analisis isi, sedangkan pendekatan kuantitatif menggunakan teknik statistik seperti regresi, uji-t, atau ANOVA.

Kedua pendekatan ini memiliki kelebihan dan kelemahan masing-masing, dan sering kali digunakan secara bersamaan untuk mendapatkan gambaran yang lebih lengkap tentang suatu fenomena.

Taken together, these form the core of the mathematical concepts needed to begin down the path towards a deep understanding of deep learning.


taken : diambel / mengabil
together : bersama - sama
these : ini
form : membentuk
path : jalan
part : bagian
towards : menuju


secara bersama-sama, ini membentuk inti konsep matematika yang diperlukan untuk memulai perjalanan menuju pemahaman yang dalam tentang deep learning.

## 22.1. Geometry and Linear Algebraic Operations (Operasi Geometri dan aljabar linier)

In Section 2.3, we encountered the basics of linear algebra and saw how it could be used to express common operations for transforming our data. Linear algebra is one of the key mathematical pillars underlying much of the work that we do in deep learning and in machine learning more broadly. While Section 2.3 contained enough machinery to communicate the mechanics of modern deep learning models, there is a lot more to the subject. In this section, we will go deeper, highlighting some geometric interpretations of linear algebra operations, and introducing a few fundamental concepts, including of eigenvalues and eigenvectors.

Pada bagian 2.3, kita mempelajari dasar-dasar aljabar linear dan bagaimana itu dapat digunakan untuk melakukan operasi yang umum dalam mengubah data kita. Aljabar linear adalah salah satu dasar matematika yang sangat penting dalam banyak pekerjaan yang kita lakukan di deep learning dan machine learning secara luas. Meskipun bagian 2.3 sudah cukup untuk menjelaskan mekanisme dari model deep learning modern, tetapi sebenarnya masih banyak lagi yang bisa dipelajari dalam topik ini. Di bagian ini, kita akan mempelajari lebih dalam lagi, dengan menyoroti beberapa interpretasi geometri dari operasi aljabar linear, serta memperkenalkan beberapa konsep dasar, termasuk nilai dan vektor eigen. Semoga membantu!

Geometri adalah cabang matematika yang mempelajari hubungan antara bentuk, ukuran, posisi, dan ruang. Secara umum, geometri membahas tentang sifat-sifat objek dalam ruang, seperti bidang, bangun ruang, dan kurva.

Geometri dapat dibagi menjadi beberapa sub-bidang, antara lain:

- Geometri Euclidean: mempelajari sifat-sifat objek dalam ruang tiga dimensi, seperti bidang, prisma, dan bola.
- Geometri analitik: menggabungkan geometri dengan aljabar untuk mempelajari objek dalam ruang tiga dimensi dengan menggunakan koordinat.
- Geometri topologi: mempelajari sifat-sifat objek yang tidak berubah meskipun objek tersebut ditarik atau diputar, seperti simpul, permukaan, dan ruang.
- Geometri diferensial: mempelajari sifat-sifat objek yang berubah secara halus dalam ruang, seperti kurva dan permukaan.


Geometri memiliki banyak aplikasi dalam kehidupan sehari-hari, seperti dalam arsitektur, teknologi, dan ilmu fisika. Dalam arsitektur, geometri digunakan untuk merancang bangunan yang efisien dan estetis. Dalam teknologi, geometri digunakan dalam pemodelan 3D dan pengembangan perangkat lunak. Sedangkan dalam ilmu fisika, geometri digunakan untuk memodelkan fenomena alam seperti gerak planet, gelombang elektromagnetik, dan lubang hitam.

### 22.1.1. Geometry of Vectors

First, we need to discuss the two common geometric interpretations of vectors, as either points or directions in space. Fundamentally, a vector is a list of numbers such as the Python list below.

Pertama-tama, kita perlu membahas dua interpretasi geometri umum dari vektor, sebagai titik atau arah di dalam ruang. Pada dasarnya, vektor adalah daftar angka seperti pada contoh di bawah ini yang biasanya dituliskan dalam format Python list.

```python
v = [1, 7, 0, 1]
```

Mathematicians most often write this as either a column or row vector, which is to say either as

Biasanya, para ahli matematika menuliskannya sebagai vektor kolom atau baris, artinya vektor tersebut dituliskan secara vertikal (kolom) atau horizontal (baris).

```math
\begin{split}\mathbf{x} = \begin{bmatrix}1\\7\\0\\1\end{bmatrix},\end{split}
```
or 

```math
\mathbf{x}^\top = \begin{bmatrix}1 & 7 & 0 & 1\end{bmatrix}.
```

These often have different interpretations, where data examples are column vectors and weights used to form weighted sums are row vectors. However, it can be beneficial to be flexible. As we have described in Section 2.3, though a single vector’s default orientation is a column vector, for any matrix representing a tabular dataset, treating each data example as a row vector in the matrix is more conventional.

Biasanya, terdapat dua interpretasi berbeda, di mana contoh data diberikan dalam bentuk vektor kolom dan bobot yang digunakan untuk membentuk jumlah terbobot dalam bentuk vektor baris. Namun, berguna untuk fleksibel. Seperti yang telah dijelaskan di Bagian 2.3, meskipun orientasi bawaan dari suatu vektor adalah vektor kolom, untuk setiap matriks yang mewakili dataset tabel, memperlakukan setiap contoh data sebagai vektor baris di dalam matriks adalah lebih umum.

Given a vector, the first interpretation that we should give it is as a point in space. In two or three dimensions, we can visualize these points by using the components of the vectors to define the location of the points in space compared to a fixed reference called the origin. This can be seen in Fig. 22.1.1.*


Ketika diberikan sebuah vektor, interpretasi pertama yang harus diberikan adalah sebagai titik di dalam ruang. Di dalam dua atau tiga dimensi, kita dapat memvisualisasikan titik-titik ini dengan menggunakan komponen dari vektor tersebut untuk menentukan lokasi titik-titik tersebut di ruang relatif terhadap suatu referensi tetap yang disebut asal titik. Hal ini dapat dilihat pada Gambar 22.1.1.

![Alt Text](https://d2l.ai/_images/grid-points.svg)

Fig. 22.1.1 An illustration of visualizing vectors as points in the plane. The first component of the vector gives the x-coordinate, the second component gives the y-coordinate. Higher dimensions are analogous, although much harder to visualize.

Gambar 22.1.1. adalah ilustrasi visualisasi vektor sebagai titik di dalam bidang. Komponen pertama dari vektor memberikan koordinat x, sementara komponen kedua memberikan koordinat y. Dimensi yang lebih tinggi serupa, walaupun lebih sulit untuk divisualisasikan.

This geometric point of view allows us to consider the problem on a more abstract level. No longer faced with some insurmountable seeming problem like classifying pictures as either cats or dogs, we can start considering tasks abstractly as collections of points in space and picturing the task as discovering how to separate two distinct clusters of points.

Dalam pandangan geometri ini, kita dapat mempertimbangkan masalah pada tingkat yang lebih abstrak. Kita tidak lagi dihadapkan dengan masalah yang tampaknya sulit seperti mengklasifikasikan gambar sebagai kucing atau anjing, melainkan kita dapat mulai mempertimbangkan tugas secara abstrak sebagai kumpulan titik-titik dalam ruang dan memvisualisasikan tugas tersebut sebagai menemukan cara memisahkan dua kelompok titik yang berbeda.

In parallel, there is a second point of view that people often take of vectors: as directions in space. Not only can we think of the vector $\mathbf{v} = [3,2]^\top$ as the location  units to the right 2 and units up from the origin, we can also think of it as the direction itself to take  3 steps to the right and 2 steps up. In this way, we consider all the vectors in figure Fig. 22.1.2 the same."

Secara paralel, ada sudut pandang kedua yang sering diambil orang tentang vektor: sebagai arah di ruang. Tidak hanya kita dapat menganggap vektor $\mathbf{v} = [3,2]^\top$ sebagai satuan lokasi ke kanan 3 dan ke atas 2 dari asal, kita juga dapat memandangnya sebagai arah itu sendiri untuk mengambil 3 langkah ke kanan dan 2 langkah ke atas. Dengan cara ini, kita menganggap semua vektor dalam gambar Fig. 22.1.2 sama.

![Alt Text](https://d2l.ai/_images/par-vec.svg)


Gambar 22.1.2 menunjukkan bahwa setiap vektor dapat divisualisasikan sebagai sebuah panah pada bidang datar. Dalam hal ini, setiap vektor yang digambarkan adalah representasi dari vektor $\mathbf{v} = [3,2]^\top$ 

One of the benefits of this shift is that we can make visual sense of the act of vector addition. In particular, we follow the directions given by one vector, and then follow the directions given by the other, as is seen in Fig. 22.1.3.

Salah satu manfaat dari pergeseran pandangan ini adalah kita dapat membuat pemahaman visual tentang operasi penjumlahan vektor. Secara khusus, kita mengikuti arah yang diberikan oleh satu vektor, dan kemudian mengikuti arah yang diberikan oleh vektor lainnya, seperti yang terlihat pada Gambar 22.1.3.

![](https://d2l.ai/_images/vec-add.svg)

Fig. 22.1.3 We can visualize vector addition by first following one vector, and then another.

Fig. 22.1.3 dapat divisualisasikan dengan mengikuti vektor pertama, lalu vektor kedua untuk menjumlahkannya.

Vector subtraction has a similar interpretation. By considering the identity that $\mathbf{u} = \mathbf{v} + (\mathbf{u}-\mathbf{v})$, we see that the vector $\mathbf{u}-\mathbf{v}$ is the direction that takes us from the point $\mathbf{v}$ to the point $\mathbf{u}$.

Vector pengurangan memiliki interpretasi yang serupa. Dengan mempertimbangkan identitas $\mathbf{u} = \mathbf{v} + (\mathbf{u}-\mathbf{v})$, kita dapat melihat bahwa vektor $\mathbf{u}-\mathbf{v}$ adalah arah yang membawa kita dari titik $\mathbf{v}$ ke titik $\mathbf{u}$.

### 22.1.2. Dot Products and Angles

As we saw in Section 2.3, if we take two column vectors  $\mathbf{u}$ and $\mathbf{u}$ , we can form their dot product by computing:

As we saw in Section 2.3, if we take two column vectors  $\mathbf{u}$ and $\mathbf{u}$ , we can form their dot product by computing:

$\mathbf{u}^\top\mathbf{v} = \sum_i u_i\cdot v_i.$

Because (22.1.3) is symmetric, we will mirror the notation of classical multiplication and write

Karena (22.1.3) bersifat simetris, kita akan meniru notasi perkalian klasik dan menulis:

$\mathbf{u}\cdot\mathbf{v} = \mathbf{u}^\top\mathbf{v} = \mathbf{v}^\top\mathbf{u},$

to highlight the fact that exchanging the order of the vectors will yield the same answer.

Hal ini dilakukan untuk menekankan bahwa menukar urutan vektor akan menghasilkan jawaban yang sama.

The dot product (22.1.3) also admits a geometric interpretation: it is closely related to the angle between two vectors. Consider the angle shown in Fig. 22.1.4.

Hasil kali titik (22.1.3) juga memiliki interpretasi geometri: itu berkaitan erat dengan sudut antara dua vektor. Perhatikan sudut yang ditunjukkan pada Gambar 22.1.4.

![](https://d2l.ai/_images/vec-angle.svg)

Fig. 22.1.4 Between any two vectors in the plane there is a well defined angle $\theta$. We will see this angle is intimately tied to the dot product.

Gambar 22.1.4, antara dua vektor dalam bidang, ada sudut $\theta$ yang didefinisikan dengan baik. Kita akan melihat bahwa sudut ini erat terkait dengan hasil kali titik.

To start, let’s consider two specific vectors:

Untuk memulai, mari kita pertimbangkan dua vektor spesifik:

$\mathbf{v} = (r,0) \; \text{and} \; \mathbf{w} = (s\cos(\theta), s \sin(\theta)).$

The vector $\mathbf{v}$ is length r and runs parallel to the x-axis, and the vector $\mathbf{w}$ is of length s and at angle $\theta$ with the x-axis. If we compute the dot product of these two vectors, we see that

Vektor $\mathbf{v}$ memiliki panjang r dan sejajar dengan sumbu x, dan vektor $\mathbf{w}$ memiliki panjang s dan membentuk sudut $\theta$ dengan sumbu x. Jika kita menghitung hasil kali titik dari kedua vektor ini, maka akan diperoleh:

$\mathbf{v}\cdot\mathbf{w} = rs\cos(\theta) = \|\mathbf{v}\|\|\mathbf{w}\|\cos(\theta).$

With some simple algebraic manipulation, we can rearrange terms to obtain

Dengan sedikit manipulasi aljabar, kita dapat mengatur ulang istilah untuk memperoleh:

$\theta = \arccos\left(\frac{\mathbf{v}\cdot\mathbf{w}}{\|\mathbf{v}\|\|\mathbf{w}\|}\right).$

In short, for these two specific vectors, the dot product combined with the norms tell us the angle between the two vectors. This same fact is true in general. We will not derive the expression here, however, if we consider writing $\|\mathbf{v} - \mathbf{w}\|^2$ in two ways: one with the dot product, and the other geometrically using the law of cosines, we can obtain the full relationship. Indeed, for any two vectors $\mathbf{v}$ and $\mathbf{w}$, the angle between the two vectors is

$\theta = \arccos\left(\frac{\mathbf{v}\cdot\mathbf{w}}{\|.\mathbf{v}\|\|\mathbf{w}\|}\right).$

This is a nice result since nothing in the computation references two-dimensions. Indeed, we can use this in three or three million dimensions without issue.

Hasil ini bagus karena tidak ada yang mengacu pada dua dimensi dalam perhitungan. Sebenarnya, kita dapat menggunakan ini dalam tiga atau tiga juta dimensi tanpa masalah.

As a simple example, let’s see how to compute the angle between a pair of vectors:

Sebagai contoh sederhana, mari kita lihat bagaimana cara menghitung sudut antara sepasang vektor:

```python
%matplotlib inline
import tensorflow as tf
from IPython import display
from d2l import tensorflow as d2l


def angle(v, w):
    return tf.acos(tf.tensordot(v, w, axes=1) / (tf.norm(v) * tf.norm(w)))

angle(tf.constant([0, 1, 2], dtype=tf.float32), tf.constant([2.0, 3, 4]))
```
```python
<tf.Tensor: shape=(), dtype=float32, numpy=0.41899002>
```
We will not use it right now, but it is useful to know that we will refer to vectors for which the angle is $\pi/2$ (or equivalently $90^{\circ}$) as being orthogonal. By examining the equation above, we see that this happens when $\theta = \pi/2$, which is the same thing as $\cos(\theta) = 0$. The only way this can happen is if the dot product itself is zero, and two vectors are orthogonal if and only if $\mathbf{v}\cdot\mathbf{w} = 0$. This will prove to be a helpful formula when understanding objects geometrically.

Sekarang kita tidak akan menggunakannya, tetapi penting untuk tahu bahwa kita akan menyebut vektor yang sudutnya $\pi/2$ (atau sama dengan 90 derajat) sebagai ortogonal. Dari persamaan di atas, kita bisa melihat bahwa ini terjadi saat $\theta = \pi/2$, yang sama dengan $\cos(\theta) = 0$. Satu-satunya cara ini terjadi adalah jika hasil kali dot-nya nol, dan dua vektor dikatakan ortogonal jika dan hanya jika $\mathbf{v}\cdot\mathbf{w} = 0$. Rumus ini akan membantu kita dalam memahami objek secara geometri.

It is reasonable to ask: why is computing the angle useful? The answer comes in the kind of invariance we expect data to have. Consider an image, and a duplicate image, where every pixel value is the same but $10\%$ the brightness. The values of the individual pixels are in general far from the original values. Thus, if one computed the distance between the original image and the darker one, the distance can be large. However, for most ML applications, the content is the same—it is still an image of a cat as far as a cat/dog classifier is concerned. However, if we consider the angle, it is not hard to see that for any vector $\mathbf{v}$, the angle between $\mathbf{v}$ and $0.1\cdot\mathbf{v}$ is zero. This corresponds to the fact that scaling vectors keeps the same direction and just changes the length. The angle considers the darker image identical.

Mungkin kamu bertanya-tanya, mengapa menghitung sudut itu berguna? Jawabannya terletak pada jenis invarian data yang kita harapkan. Misalkan kita memiliki sebuah gambar, dan sebuah duplikat dari gambar tersebut, dimana setiap nilai piksel sama, tetapi nilainya hanya $10%$ dari kecerahan aslinya. Nilai piksel individu pada kedua gambar tersebut pada umumnya jauh dari nilai aslinya. Jadi, jika kita menghitung jarak antara gambar asli dan gambar yang lebih gelap tersebut, jaraknya bisa jadi sangat besar. Namun, untuk sebagian besar aplikasi pembelajaran mesin, konten pada kedua gambar tersebut sama saja - bagi klasifikasi kucing/anjing, keduanya masih merupakan gambar kucing. Namun, jika kita mempertimbangkan sudut, tidak sulit untuk melihat bahwa untuk setiap vektor $\mathbf{v}$, sudut antara $\mathbf{v}$ dan $0.1\cdot\mathbf{v}$ adalah nol. Ini berarti bahwa mengubah skala vektor hanya mempertahankan arah yang sama dan mengubah panjangnya saja. Sudut mempertimbangkan gambar yang lebih gelap sebagai identik dengan gambar asli.

Contoh seperti ini dapat ditemukan di mana-mana. Dalam teks, mungkin kita ingin topik yang dibahas tidak berubah jika kita menulis dokumen yang dua kali lebih panjang yang berisi hal yang sama. Untuk beberapa teknik enkoding (seperti menghitung jumlah kemunculan kata dalam beberapa kosakata), ini berarti menggandakan vektor yang merepresentasikan dokumen, sehingga kita dapat menggunakan sudut kembali.

### 22.1.2.1. Cosine Similarity
In ML contexts where the angle is employed to measure the closeness of two vectors, practitioners adopt the term cosine similarity to refer to the portion

$\cos(\theta) = \frac{\mathbf{v}\cdot\mathbf{w}}{\|\mathbf{v}\|\|\mathbf{w}\|}.$

Dalam konteks pembelajaran mesin di mana sudut digunakan untuk mengukur kedekatan dua vektor, para praktisi mengadopsi istilah kesamaan kosinus untuk merujuk pada bagian dari sudut yang membentuk kesamaan antara vektor-vektor tersebut.

The cosine takes a maximum value of 1 when the two vectors point in the same direction, a minimum value of 
-1 when they point in opposite directions, and a value of 
0 when the two vectors are orthogonal. Note that if the components of high-dimensional vectors are sampled randomly with mean, their cosine will nearly always be close to 0.

Kosinus memiliki nilai maksimum 1 ketika dua vektor menunjuk ke arah yang sama, nilai minimum -1 ketika mereka menunjuk ke arah yang berlawanan, dan nilai 0 ketika dua vektor bersifat ortogonal. Perhatikan bahwa jika komponen-komponen vektor dengan dimensi tinggi diambil secara acak dengan rata-rata, maka nilainya akan cenderung mendekati 0.

### 22.1.3. Hyperplanes

In addition to working with vectors, another key object that you must understand to go far in linear algebra is the hyperplane, a generalization to higher dimensions of a line (two dimensions) or of a plane (three dimensions). In an d-dimensional vector space, a hyperplane has $d-1$ dimensions and divides the space into two half-spaces.

Selain bekerja dengan vektor, objek penting lain yang harus dipahami untuk mempelajari aljabar linear secara lebih mendalam adalah hiperruang, yaitu generalisasi untuk dimensi yang lebih tinggi dari suatu garis (dua dimensi) atau bidang (tiga dimensi). Dalam ruang vektor d-dimensi, suatu hiperruang memiliki dimensi $d-1$ dan membagi ruang menjadi dua setengah-ruang.

Let’s start with an example. Suppose that we have a column vector $\mathbf{w}=[2,1]^\top$. We want to know, “what are the points $\mathbf{v}$ with $\mathbf{w}\cdot\mathbf{v} = 1$?” By recalling the connection between dot products and angles above (22.1.8), we can see that this is equivalent to

Mari kita mulai dengan contoh. Misalkan kita memiliki vektor kolom $\mathbf{w}=[2,1]^\top$. Kita ingin tahu, "apa saja titik $\mathbf{v}$ yang memenuhi syarat $\mathbf{w}\cdot\mathbf{v}=1$?" Dengan mengingat hubungan antara dot product dan sudut di atas (22.1.8), kita dapat melihat bahwa ini sama dengan:

$\|\mathbf{v}\|\|\mathbf{w}\|\cos(\theta) = 1 \; \iff \; \|\mathbf{v}\|\cos(\theta) = \frac{1}{\|\mathbf{w}\|} = \frac{1}{\sqrt{5}}.$

![](https://d2l.ai/_images/proj-vec.svg)

Fig. 22.1.5 Recalling trigonometry, we see the formula $\|\mathbf{v}\|\cos(\theta)$ is the length of the projection of the vector v onto the direction of w

Gambar 22.1.5. Dengan mengingat pelajaran tentang trigonometri, kita dapat melihat bahwa rumus $|\mathbf{v}|\cos(\theta)$ adalah panjang proyeksi vektor $\mathbf{v}$ pada arah dari $\mathbf{w}$.


If we consider the geometric meaning of this expression, we see that this is equivalent to saying that the length of the projection of v onto the direction of w is exactly $1/\|\mathbf{w}\|$, as is shown in Fig. 22.1.5. The set of all points where this is true is a line at right angles to the vector w. If we wanted, we could find the equation for this line and see that it is $2x + y = 1$ or equivalently $y = 1 - 2x$.

Jika kita mempertimbangkan makna geometris dari ungkapan ini, kita akan melihat bahwa ini sama dengan mengatakan bahwa panjang proyeksi v ke arah w sama dengan $1/|\mathbf{w}|$, seperti yang ditunjukkan pada Gambar 22.1.5. Himpunan semua titik di mana ini benar adalah sebuah garis yang tegak lurus terhadap vektor w. Jika kita ingin, kita dapat menemukan persamaan untuk garis ini dan melihat bahwa itu adalah $2x+y=1$ atau sama dengan $y=1-2x$.

If we now look at what happens when we ask about the set of points with $\mathbf{w}\cdot\mathbf{v} > 1$ or $\mathbf{w}\cdot\mathbf{v} < 1$, we can see that these are cases where the projections are longer or shorter than $1/\|\mathbf{w}\|$, respectively. Thus, those two inequalities define either side of the line. In this way, we have found a way to cut our space into two halves, where all the points on one side have dot product below a threshold, and the other side above as we see in Fig. 22.1.6.

![](https://d2l.ai/_images/space-division.svg)

Fig. 22.1.6 If we now consider the inequality version of the expression, we see that our hyperplane (in this case: just a line) separates the space into two halves.

Gambar 22.1.6 Jika kita sekarang mempertimbangkan versi ketidaksamaan dari ekspresi tersebut, kita dapat melihat bahwa hiperrata (dalam kasus ini: hanya sebuah garis) membagi ruang menjadi dua bagian.

The story in higher dimension is much the same. If we now take $\mathbf{w} = [1,2,3]^\top$ and ask about the points in three dimensions with $\mathbf{w}\cdot\mathbf{v} = 1$, we obtain a plane at right angles to the given vector w. The two inequalities again define the two sides of the plane as is shown in Fig. 22.1.7.

Jika kita memperluas konsep ini ke dimensi yang lebih tinggi, ceritanya hampir sama. Misalnya, jika kita memiliki vektor $\mathbf{w}=[1,2,3]^\top$ dan mencari titik-titik dalam tiga dimensi dengan $\mathbf{w}\cdot\mathbf{v} = 1$, kita akan mendapatkan sebuah bidang tegak lurus terhadap vektor $\mathbf{w}$. Dua ketidaksetaraan kembali mendefinisikan kedua sisi bidang tersebut, seperti yang ditunjukkan pada Gambar 22.1.7.

![](https://d2l.ai/_images/space-division-3d.svg)

Fig. 22.1.7 Hyperplanes in any dimension separate the space into two halves.

Gambar 22.1.7 menjelaskan bahwa hyperplane pada dimensi apapun membagi ruang menjadi dua bagian.

While our ability to visualize runs out at this point, nothing stops us from doing this in tens, hundreds, or billions of dimensions. This occurs often when thinking about machine learned models. For instance, we can understand linear classification models like those from Section 4.1, as methods to find hyperplanes that separate the different target classes. In this context, such hyperplanes are often referred to as decision planes. The majority of deep learned classification models end with a linear layer fed into a softmax, so one can interpret the role of the deep neural network to be to find a non-linear embedding such that the target classes can be separated cleanly by hyperplanes.

Kemampuan visual kita sudah tidak mencukupi saat kita berurusan dengan dimensi yang sangat tinggi, seperti puluhan, ratusan, atau miliaran dimensi. Namun, ini sering terjadi saat berpikir tentang model yang dipelajari oleh mesin. Sebagai contoh, kita dapat memahami model klasifikasi linear seperti pada bagian 4.1, sebagai metode untuk menemukan hiperbidang yang memisahkan kelas target yang berbeda. Dalam konteks ini, hiperbidang tersebut sering disebut sebagai decision plane. Sebagian besar model klasifikasi yang didalaminya terdapat deep neural network berakhir dengan lapisan linear yang disusul oleh softmax, sehingga kita dapat memahami peran jaringan saraf dalam menemukan embedding non-linear sehingga kelas target dapat dipisahkan dengan jelas oleh hiperbidang.

To give a hand-built example, notice that we can produce a reasonable model to classify tiny images of t-shirts and trousers from the Fashion-MNIST dataset (seen in Section 4.2) by just taking the vector between their means to define the decision plane and eyeball a crude threshold. First we will load the data and compute the averages.

Sebagai contoh yang dibuat secara manual, kita dapat membuat model yang cukup baik untuk mengklasifikasikan gambar-gambar kecil baju dan celana dari dataset Fashion-MNIST (seperti yang terlihat pada bagian 4.2) dengan hanya mengambil vektor antara rata-rata mereka untuk menentukan decision plane dan memperkirakan ambang batas secara kasar. Pertama, kita akan memuat data dan menghitung rata-ratanya.

```python
# Load in the dataset
((train_images, train_labels), (
    test_images, test_labels)) = tf.keras.datasets.fashion_mnist.load_data()


X_train_0 = tf.cast(tf.stack(train_images[[i for i, label in enumerate(
    train_labels) if label == 0]] * 256), dtype=tf.float32)
X_train_1 = tf.cast(tf.stack(train_images[[i for i, label in enumerate(
    train_labels) if label == 1]] * 256), dtype=tf.float32)
X_test = tf.cast(tf.stack(test_images[[i for i, label in enumerate(
    test_labels) if label == 0]] * 256), dtype=tf.float32)
y_test = tf.cast(tf.stack(test_images[[i for i, label in enumerate(
    test_labels) if label == 1]] * 256), dtype=tf.float32)

# Compute averages
ave_0 = tf.reduce_mean(X_train_0, axis=0)
ave_1 = tf.reduce_mean(X_train_1, axis=0)
```

It can be informative to examine these averages in detail, so let’s plot what they look like. In this case, we see that the average indeed resembles a blurry image of a t-shirt.

Melihat rata-rata dapat memberikan informasi penting, sehingga mari kita lihat gambarnya lebih detail. Dalam hal ini, mari kita gambarkan seperti apa rata-rata tersebut. Dalam kasus ini, kita bisa melihat bahwa rata-rata memang menyerupai gambar kabur dari sebuah kaos.

```python
# Plot average t-shirt
d2l.set_figsize()
d2l.plt.imshow(tf.reshape(ave_0, (28, 28)), cmap='Greys')
d2l.plt.show()
```
![](https://d2l.ai/_images/output_geometry-linear-algebraic-ops_80b1c3_45_0.svg)


In the second case, we again see that the average resembles a blurry image of trousers.

Dalam kasus kedua, kita melihat bahwa rerata memang menyerupai gambar celana panjang yang buram.

```python
# Plot average trousers
d2l.plt.imshow(tf.reshape(ave_1, (28, 28)), cmap='Greys')
d2l.plt.show()
```

![](https://d2l.ai/_images/output_geometry-linear-algebraic-ops_80b1c3_57_0.svg)

In a fully machine learned solution, we would learn the threshold from the dataset. In this case, I simply eyeballed a threshold that looked good on the training data by hand.

Dalam sebuah solusi yang sepenuhnya dipelajari oleh mesin, kita akan belajar menentukan ambang batas dari data set. Dalam kasus ini, saya hanya secara manual menentukan ambang batas yang terlihat baik pada data pelatihan dengan memperkirakan tanpa alat bantu.

```python
# Print test set accuracy with eyeballed threshold
w = tf.transpose(ave_1 - ave_0)
predictions = tf.reduce_sum(X_test * tf.nest.flatten(w), axis=0) > -1500000

# Accuracy
tf.reduce_mean(
    tf.cast(tf.cast(predictions, y_test.dtype) == y_test, tf.float32))
```
```python
<tf.Tensor: shape=(), dtype=float32, numpy=0.4602704>
```

### 22.1.4. Geometry of Linear Transformations¶
Through Section 2.3 and the above discussions, we have a solid understanding of the geometry of vectors, lengths, and angles. However, there is one important object we have omitted discussing, and that is a geometric understanding of linear transformations represented by matrices. Fully internalizing what matrices can do to transform data between two potentially different high dimensional spaces takes significant practice, and is beyond the scope of this appendix. However, we can start building up intuition in two dimensions.

Di bagian 2.3 dan diskusi di atas, kita memiliki pemahaman yang kuat tentang geometri vektor, panjang, dan sudut. Namun, ada satu objek penting yang belum kita bahas, yaitu pemahaman geometris dari transformasi linear yang direpresentasikan oleh matriks. Memahami sepenuhnya apa yang dapat dilakukan matriks untuk mentransformasikan data antara dua ruang berdimensi tinggi yang berbeda membutuhkan latihan yang signifikan, dan melampaui cakupan lampiran ini. Namun, kita dapat mulai membangun intuisi dalam dua dimensi.

Suppose that we have some matrix:
Anggaplah kita memiliki suatu matriks:

$\begin{split}\mathbf{A} = \begin{bmatrix}
a & b \\ c & d
\end{bmatrix}.\end{split}$

If we want to apply this to an arbitrary vector $\mathbf{v} = [x, y]^\top$, we multiply and see that

Jika kita ingin menerapkan hal ini pada vektor sembarang $\mathbf{v} = [x, y]^\top$, kita melakukan perkalian dan dapat dilihat bahwa...

```math
\begin{split}\begin{aligned}
\mathbf{A}\mathbf{v} & = \begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} \\
& = \begin{bmatrix}ax+by\\ cx+dy\end{bmatrix} \\
& = x\begin{bmatrix}a \\ c\end{bmatrix} + y\begin{bmatrix}b \\d\end{bmatrix} \\
& = x\left\{\mathbf{A}\begin{bmatrix}1\\0\end{bmatrix}\right\} + y\left\{\mathbf{A}\begin{bmatrix}0\\1\end{bmatrix}\right\}.
\end{aligned}\end{split}
```

This may seem like an odd computation, where something clear became somewhat impenetrable. However, it tells us that we can write the way that a matrix transforms any vector in terms of how it transforms two specific vectors: $[1,0]^\top$ and $[0,1]^\top$. This is worth considering for a moment. We have essentially reduced an infinite problem (what happens to any pair of real numbers) to a finite one (what happens to these specific vectors). These vectors are an example a basis, where we can write any vector in our space as a weighted sum of these basis vectors.

Let’s draw what happens when we use the specific matrix

Hal ini mungkin tampak seperti perhitungan yang aneh, di mana sesuatu yang jelas menjadi agak sulit dipahami. Namun, hal ini memberi tahu kita bahwa kita dapat menulis cara suatu matriks mentransformasi vektor dalam hal bagaimana ia mentransformasi dua vektor tertentu: $[1, 0]^\top$ dan $[0, 1]^\top$. Hal ini layak dipertimbangkan sejenak. Kita pada dasarnya telah mengurangi masalah yang tak terhingga (apa yang terjadi pada pasangan angka riil apa pun) menjadi masalah yang terbatas (apa yang terjadi pada vektor basis ini). Vektor-vektor ini adalah contoh basis, di mana kita dapat menulis setiap vektor dalam ruang kita sebagai jumlah tertimbang dari vektor basis ini.

Mari kita gambar apa yang terjadi ketika kita menggunakan matriks tertentu.

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
1 & 2 \\
-1 & 3
\end{bmatrix}.\end{split}
```

If we look at the specific vector $\mathbf{v} = [2, -1]^\top$, we see this is $2\cdot[1,0]^\top + -1\cdot[0,1]^\top$, and thus we know that the matrix will send this to 
$2(\mathbf{A}[1,0]^\top) + -1(\mathbf{A}[0,1])^\top = 2[1, -1]^\top - [2,3]^\top = [0, -5]^\top$. If we follow this logic through carefully, say by considering the grid of all integer pairs of points, we see that what happens is that the matrix multiplication can skew, rotate, and scale the grid, but the grid structure must remain as you see in Fig. 22.1.8.

Jika kita melihat vektor spesifik $\mathbf{v} = [2, -1]^\top$, kita dapat menuliskannya sebagai $2\cdot[1,0]^\top + -1\cdot[0,1]^\top$, sehingga kita tahu bahwa matriks akan mengirimnya ke $2(\mathbf{A}[1,0]^\top) + -1(\mathbf{A}[0,1])^\top = 2[1, -1]^\top - [2,3]^\top = [0, -5]^\top. Jika kita mengikuti logika ini dengan hati-hati, misalnya dengan mempertimbangkan kisi-kisi dari semua pasangan bilangan bulat, kita dapat melihat bahwa hasil perkalian matriks dapat memutar, memutar tegak lurus, dan mengubah skala kisi-kisi, tetapi struktur kisi-kisi harus tetap sama seperti yang terlihat pada Gambar 22.1.8."

![](https://d2l.ai/_images/grid-transform.svg)

Fig. 22.1.8 The matrix A acting on the given basis vectors. Notice how the entire grid is transported along with it.

Gambar 22.1.8 menunjukkan bagaimana matriks A memengaruhi vektor basis yang diberikan. Perhatikan bagaimana seluruh grid tertransportasi bersama dengan vektornya.

This is the most important intuitive point to internalize about linear transformations represented by matrices. Matrices are incapable of distorting some parts of space differently than others. All they can do is take the original coordinates on our space and skew, rotate, and scale them.

Some distortions can be severe. For instance the matrix

Ini adalah titik intuisi yang paling penting untuk dipahami tentang transformasi linear yang direpresentasikan oleh matriks. Matriks tidak mampu memutarbagian-bagian ruang yang berbeda-beda. Yang dapat dilakukannya hanyalah mengambil koordinat asli pada ruang kita dan menariknya, memutar, dan mengubah ukurannya.

Beberapa distorsi bisa sangat parah. Misalnya, matriks

```math
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & -1 \\ 4 & -2
\end{bmatrix},\end{split}
```

compresses the entire two-dimensional plane down to a single line. Identifying and working with such transformations are the topic of a later section, but geometrically we can see that this is fundamentally different from the types of transformations we saw above. For instance, the result from matrix A can be “bent back” to the original grid. The results from matrix B cannot because we will never know where the vector $[1,2]^\top$ came from—was it $[1,1]^\top$ or $[0, -1]^\top$?

Pada intinya, kita perlu memahami bahwa matriks hanya bisa memutar, memperbesar, dan memiringkan setiap titik di ruang koordinat, tanpa mengubah bentuk ruang koordinat itu sendiri. Namun, terdapat beberapa jenis transformasi yang dapat merubah bentuk ruang koordinat yang kita bahas pada bagian lain. Sebagai contoh, matriks A hanya memampatkan bidang 2 dimensi menjadi satu garis. Namun, hasil dari matriks A masih dapat "dibengkokkan kembali" ke bentuk grid asli, sedangkan hasil dari matriks B tidak dapat dikembalikan karena kita tidak tahu asal dari vektor $[1,2]^\top$ apakah berasal dari $[1,1]^\top$ atau $[0, -1]^\top$.

While this picture was for a 2 x 2 matrix, nothing prevents us from taking the lessons learned into higher dimensions. If we take similar basis vectors like $[1,0, \ldots,0]$ and see where our matrix sends them, we can start to get a feeling for how the matrix multiplication distorts the entire space in whatever dimension space we are dealing with.

Dalam gambar ini, meskipun gambar tersebut menggambarkan sebuah matriks 2 x 2, tetapi kita bisa mengambil pelajaran yang sama ketika menerapkannya pada dimensi yang lebih tinggi. Jika kita menggunakan vektor basis yang serupa seperti $[1,0, \ldots,0]$ dan melihat ke mana matriks mengirimkannya, kita dapat mulai merasakan bagaimana perkalian matriks tersebut akan memutar seluruh ruang dalam dimensi apapun yang sedang kita hadapi.

### 22.1.5. Linear Dependence

Consider again the matrix
Pertimbangkan kembali matriks tersebut

```Math
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & -1 \\ 4 & -2
\end{bmatrix}.\end{split}
```

This compresses the entire plane down to live on the single line $y = 2x$. The question now arises: is there some way we can detect this just looking at the matrix itself? The answer is that indeed we can. Let’s take $y = 2x$ and $\mathbf{b}_2 = [-1, -2]^\top$ be the two columns of B.Remember that we can write everything transformed by the matrix B as a weighted sum of the columns of the matrix: $a_1\mathbf{b}_1 + a_2\mathbf{b}_2like$. We call this a linear combination. The fact that $\mathbf{b}_1 = -2\cdot\mathbf{b}_2$ means that we can write any linear combination of those two columns entirely in terms of say $\mathbf{b}_2$ since

Ini memampatkan seluruh bidang datar menjadi satu garis tunggal $y = 2x$. Pertanyaannya sekarang muncul: apakah ada cara untuk mendeteksi ini hanya dengan melihat matriks itu sendiri? Jawabannya adalah bahwa memang ada. Mari kita ambil $y = 2x$ dan $\mathbf{b}_2 = [-1, -2]^\top$ sebagai dua kolom dari matriks B. Ingat bahwa kita dapat menulis semua yang ditransformasikan oleh matriks B sebagai jumlah tertimbang dari kolom-kolom matriks tersebut: $a_1\mathbf{b}_1 + a_2\mathbf{b}_2$. Ini disebut kombinasi linear. Fakta bahwa $\mathbf{b}_1 = -2\cdot\mathbf{b}_2$ berarti bahwa kita dapat menulis setiap kombinasi linear dari kedua kolom itu sepenuhnya dalam hal $\mathbf{b}_2 saja.

$a_1\mathbf{b}_1 + a_2\mathbf{b}_2 = -2a_1\mathbf{b}_2 + a_2\mathbf{b}_2 = (a_2-2a_1)\mathbf{b}_2.$

This means that one of the columns is, in a sense, redundant because it does not define a unique direction in space. This should not surprise us too much since we already saw that this matrix collapses the entire plane down into a single line. Moreover, we see that the linear dependence $\mathbf{b}_1 = -2\cdot\mathbf{b}_2$ captures this. To make this more symmetrical between the two vectors, we will write this as

Ini berarti bahwa salah satu kolom, dalam artian tertentu, tidak diperlukan karena tidak menentukan arah unik di ruang. Hal ini seharusnya tidak terlalu mengejutkan karena kita sudah melihat bahwa matriks ini memampatkan seluruh bidang datar menjadi satu garis tunggal. Selain itu, kita melihat bahwa ketergantungan linear $\mathbf{b}_1 = -2\cdot\mathbf{b}_2$ mencerminkan hal ini. Untuk membuat ini lebih simetris antara kedua vektor, kita akan menulisnya sebagai.

$\mathbf{b}_1  + 2\cdot\mathbf{b}_2 = 0.$

In general, we will say that a collection of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_k$ are linearly dependent if there exist coefficients $a_1, \ldots, a_k$ not all equal to zero so that

Secara umum, kita akan mengatakan bahwa sebuah kumpulan vektor $\mathbf{v}_1, \ldots, \mathbf{v}_k$ merupakan bergantung linear jika terdapat koefisien $a_1, \ldots, a_k$ yang tidak semuanya sama dengan nol sehingga.

$\sum_{i=1}^k a_i\mathbf{v_i} = 0.$

In this case, we can solve for one of the vectors in terms of some combination of the others, and effectively render it redundant. Thus, a linear dependence in the columns of a matrix is a witness to the fact that our matrix is compressing the space down to some lower dimension. If there is no linear dependence we say the vectors are linearly independent. If the columns of a matrix are linearly independent, no compression occurs and the operation can be undone.

Dalam hal ini, kita dapat mencari solusi untuk salah satu vektor dalam bentuk kombinasi linear dari vektor yang lain, sehingga efektif membuatnya tidak diperlukan lagi. Oleh karena itu, ketergantungan linear pada kolom-kolom sebuah matriks menjadi bukti bahwa matriks tersebut memampatkan ruang ke dimensi yang lebih rendah. Jika tidak ada ketergantungan linear, kita katakan vektor-vektor tersebut linearly independent. Jika kolom-kolom sebuah matriks linearly independent, tidak ada pemampatan yang terjadi dan operasi dapat dibatalkan.

### 22.1.6. Rank

If we have a general $n\times m$ matrix, it is reasonable to ask what dimension space the matrix maps into. A concept known as the rank will be our answer. In the previous section, we noted that a linear dependence bears witness to compression of space into a lower dimension and so we will be able to use this to define the notion of rank. In particular, the rank of a matrix A is the largest number of linearly independent columns amongst all subsets of columns. For example, the matrix

Jika kita memiliki matriks umum berukuran $n \times m$, maka wajar untuk bertanya dalam dimensi ruang berapa matriks tersebut memetakan. Konsep yang dikenal sebagai rank akan menjadi jawaban kita. Pada bagian sebelumnya, kita mencatat bahwa ketergantungan linear menjadi bukti pemampatan ruang ke dimensi yang lebih rendah, sehingga kita dapat menggunakan hal ini untuk mendefinisikan konsep rank. Secara khusus, rank sebuah matriks A adalah jumlah terbesar dari kolom-kolom yang linearly independent di antara semua subset kolom. Sebagai contoh, matriks:

```math
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & 4 \\ -1 & -2
\end{bmatrix},\end{split}
```

has $\mathrm{rank}(B)=1$, since the two columns are linearly dependent, but either column by itself is not linearly dependent. For a more challenging example, we can consider

Matriks tersebut memiliki $\mathrm{rank}(B)=1$, karena dua kolomnya saling tergantung linear, tetapi setiap kolom secara individu tidak saling tergantung linear. Sebagai contoh yang lebih menantang, kita dapat mempertimbangkan:

```math
\begin{split}\mathbf{C} = \begin{bmatrix}
1& 3 & 0 & -1 & 0 \\
-1 & 0 & 1 & 1 & -1 \\
0 & 3 & 1 & 0 & -1 \\
2 & 3 & -1 & -2 & 1
\end{bmatrix},\end{split}
```

and show that C has rank two since, for instance, the first two columns are linearly independent, however any of the four collections of three columns are dependent.

Kita dapat menunjukkan bahwa matriks C memiliki rank dua, karena misalnya dua kolom pertama saling tergantung linear, namun empat koleksi dari tiga kolom apapun saling tergantung linear.

This procedure, as described, is very inefficient. It requires looking at every subset of the columns of our given matrix, and thus is potentially exponential in the number of columns. Later we will see a more computationally efficient way to compute the rank of a matrix, but for now, this is sufficient to see that the concept is well defined and understand the meaning.

Namun, prosedur ini sangat tidak efisien. Ini memerlukan melihat setiap subset dari kolom-kolom dari matriks yang diberikan, dan dengan demikian dapat berpotensi menjadi eksponensial dalam jumlah kolom. Nanti kita akan melihat cara yang lebih efisien secara komputasi untuk menghitung rank sebuah matriks, tetapi untuk saat ini, ini sudah cukup untuk melihat bahwa konsepnya telah terdefinisi dengan baik dan memahami artinya.

### 22.1.7. Invertibility

We have seen above that multiplication by a matrix with linearly dependent columns cannot be undone, i.e., there is no inverse operation that can always recover the input. However, multiplication by a full-rank matrix (i.e., some A that is $n \times n$ matrix with rank n), we should always be able to undo it. Consider the matrix

Seperti yang telah kita lihat sebelumnya, perkalian oleh sebuah matriks dengan kolom yang saling tergantung linear tidak dapat dibatalkan, yaitu tidak ada operasi invers yang selalu dapat memulihkan masukan awal. Namun, jika kita melakukan perkalian oleh sebuah matriks dengan rank penuh (yaitu sebuah matriks A yang merupakan matriks $n \times n$ dengan rank n), kita seharusnya selalu dapat membatalkannya. Misalkan matriks A adalah seperti berikut ini:

```math
\begin{split}\mathbf{I} = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}.\end{split}
```

which is the matrix with ones along the diagonal, and zeros elsewhere. We call this the identity matrix. It is the matrix which leaves our data unchanged when applied. To find a matrix which undoes what our matrix A has done, we want to find a matrix $\mathbf{A}^{-1}$ such that

Matrix di atas adalah matriks dengan elemen 1 di sepanjang diagonal dan 0 di tempat lain. Matriks ini disebut matriks identitas. Ini adalah matriks yang tidak mengubah data kita ketika diterapkan. Untuk mencari matriks yang membatalkan apa yang dilakukan matriks A kita ingin mencari matriks $\mathbf{A}^{-1}$ sehingga...

$\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} =  \mathbf{I}.$

If we look at this as a system, we have $n \times n$ unknowns (the entries of $\mathbf{A}^{-1}$) and equations (the equality that needs to hold between every entry of the product $\mathbf{A}^{-1}\mathbf{A}$ and every entry of I) so we should generically expect a solution to exist. Indeed, in the next section we will see a quantity called the determinant, which has the property that as long as the determinant is not zero, we can find a solution. We call such a matrix $\mathbf{A}^{-1}$ the inverse matrix. As an example, if A is the general 2 x 2 matrix

Jika kita melihatnya sebagai sistem, kita memiliki $n \times n$ variabel (entri dari $\mathbf{A}^{-1}$) dan persamaan (persamaan kesetaraan yang harus dipenuhi antara setiap entri dari hasil kali $\mathbf{A}^{-1}\mathbf{A}$ dan setiap entri dari I), sehingga kita dapat mengharapkan solusi secara umum. Memang, pada bagian berikutnya kita akan melihat sebuah bilangan yang disebut determinan, yang memiliki sifat bahwa selama determinannya tidak nol, kita dapat menemukan solusi. Kami menyebut matriks tersebut $\mathbf{A}^{-1}$ sebagai matriks balikan. Sebagai contoh, jika A adalah matriks 2 x 2 umum.

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix},\end{split}
```
then we can see that the inverse is

```math
\begin{split} \frac{1}{ad-bc}  \begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}.\end{split}
```

We can test to see this by seeing that multiplying by the inverse given by the formula above works in practice.

Kita bisa menguji apakah rumus untuk invers matriks yang diberikan di atas berhasil dengan mengalikan matriks tersebut dengan inversnya.

```python
M = tf.constant([[1, 2], [1, 4]], dtype=tf.float32)
M_inv = tf.constant([[2, -1], [-0.5, 0.5]])
tf.matmul(M_inv, M)
```
```python
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 0.],
       [0., 1.]], dtype=float32)>
```

### 22.1.7.1. Numerical Issues

While the inverse of a matrix is useful in theory, we must say that most of the time we do not wish to use the matrix inverse to solve a problem in practice. In general, there are far more numerically stable algorithms for solving linear equations like

Meskipun invers dari sebuah matriks berguna dalam teori, namun sebagian besar waktu kita tidak ingin menggunakan invers matriks untuk memecahkan masalah dalam praktek. Pada umumnya, ada banyak algoritma yang lebih stabil secara numerik untuk menyelesaikan persamaan linear seperti... (teks selanjutnya tidak diberikan)

$\mathbf{A}\mathbf{x} = \mathbf{b},$

than computing the inverse and multiplying to get

Lebih baik menggunakan metode lain untuk menyelesaikan perhitungan daripada menghitung inversi dan mengalikannya dengan matriks.

$\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}.$

Just as division by a small number can lead to numerical instability, so can inversion of a matrix which is close to having low rank.

Seperti pembagian dengan angka kecil, penginversian matriks yang hampir memiliki rank rendah juga dapat menyebabkan ketidakstabilan numerik.

Moreover, it is common that the matrix A is sparse, which is to say that it contains only a small number of non-zero values. If we were to explore examples, we would see that this does not mean the inverse is sparse. Even if A was a 1 million by 1 million matrix with only 5 million non-zero entries (and thus we need only store those 5 million), the inverse will typically have almost every entry non-negative, requiring us to store all $1\text{M}^2$ entries—that is trillion entries!

Dalam prakteknya, kita seringkali tidak ingin menggunakan invers matriks untuk menyelesaikan masalah. Selain itu, seringkali matriks A yang kita hadapi adalah matriks sparse, yang artinya hanya sedikit nilai yang tidak nol. Namun, meskipun matriks A memiliki ukuran 1 juta x 1 juta dan hanya memiliki 5 juta entri tidak nol (dan kita hanya perlu menyimpan 5 juta entri tersebut), invers dari matriks tersebut biasanya akan memiliki hampir setiap entri bernilai positif, sehingga kita harus menyimpan semua $1\text{M}^2$ entri, yaitu satu triliun entri.

While we do not have time to dive all the way into the thorny numerical issues frequently encountered when working with linear algebra, we want to provide you with some intuition about when to proceed with caution, and generally avoiding inversion in practice is a good rule of thumb.

Ketika bekerja dengan aljabar linear, seringkali muncul masalah numerik yang rumit. Oleh karena itu, kita perlu berhati-hati dan sebaiknya menghindari penggunaan invers matriks pada umumnya.

## 22.1.8. Determinant
The geometric view of linear algebra gives an intuitive way to interpret a fundamental quantity known as the determinant. Consider the grid image from before, but now with a highlighted region (Fig. 22.1.9).

Pandangan geometri dalam aljabar linear memberikan cara yang mudah dipahami untuk menafsirkan sebuah nilai dasar yang disebut dengan determinan. Bayangkan gambar grid yang telah dilihat sebelumnya, tapi sekarang ada daerah yang di-highlight (Lihat Gambar 22.1.9).

![](https://d2l.ai/_images/grid-transform-filled.svg)

Fig. 22.1.9 The matrix A again distorting the grid. This time, I want to draw particular attention to what happens to the highlighted square.

Gambar 22.1.9 menunjukkan matriks yang kembali merubah grid. Kali ini, saya ingin memperhatikan apa yang terjadi pada kotak yang di-highlight.

Look at the highlighted square. This is a square with edges given by (0,1) and (1,0) and thus it has area one. After A transforms this square, we see that it becomes a parallelogram. There is no reason this parallelogram should have the same area that we started with, and indeed in the specific case shown here of

Perhatikan kotak yang di-highlight. Ini adalah kotak dengan sisi yang diberikan oleh (0,1) dan (1,0) sehingga memiliki luas satu. Setelah A mengubah kotak ini, kita melihat bahwa kotak tersebut menjadi sebuah segi empat sama kaki. Tidak ada alasan segi empat sama kaki ini harus memiliki luas yang sama dengan kotak awal, dan memang dalam kasus spesifik yang ditunjukkan di sini dari...

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
1 & 2 \\
-1 & 3
\end{bmatrix},\end{split}
```

it is an exercise in coordinate geometry to compute the area of this parallelogram and obtain that the area is 5.

In general, if we have a matrix

Menghitung luas segi empat sama kaki ini adalah latihan geometri koordinat dan diperoleh bahwa luasnya adalah 5.

Secara umum, jika kita memiliki sebuah matriks...

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix},\end{split}
```

we can see with some computation that the area of the resulting parallelogram is $ad-bc$. This area is referred to as the determinant.

Let’s check this quickly with some example code.

Dengan melakukan beberapa perhitungan, kita dapat melihat bahwa luas segi empat sama kaki yang dihasilkan adalah $ad-bc$. Luas ini disebut sebagai determinan.

Mari kita periksa dengan cepat menggunakan beberapa contoh kode.

```python
tf.linalg.det(tf.constant([[1, -1], [2, 3]], dtype=tf.float32))
```
```python
<tf.Tensor: shape=(), dtype=float32, numpy=5.0>
```

The eagle-eyed amongst us will notice that this expression can be zero or even negative. For the negative term, this is a matter of convention taken generally in mathematics: if the matrix flips the figure, we say the area is negated. Let’s see now that when the determinant is zero, we learn more.

Let’s consider

Mereka yang memperhatikan dengan teliti akan menyadari bahwa ungkapan ini bisa nol atau bahkan negatif. Untuk istilah negatif, ini adalah masalah konvensi yang umum diambil dalam matematika: jika matriks membalikkan bentuknya, kita mengatakan luasnya dinotasikan sebagai negatif. Sekarang mari kita lihat bahwa ketika determinan adalah nol, kita bisa belajar lebih banyak.

Mari kita pertimbangkan...

```python
\begin{split}\mathbf{B} = \begin{bmatrix}
2 & 4 \\ -1 & -2
\end{bmatrix}.\end{split}
```

If we compute the determinant of this matrix, we get $2\cdot(-2 ) - 4\cdot(-1) = 0$. Given our understanding above, this makes sense. B compresses the square from the original image down to a line segment, which has zero area. And indeed, being compressed into a lower dimensional space is the only way to have zero area after the transformation. Thus we see the following result is true: a matrix A is invertible if and only if the determinant is not equal to zero.

Jika kita menghitung determinan matriks ini, maka kita akan mendapatkan $2\cdot(-2 ) - 4\cdot(-1) = 0$. Berdasarkan pemahaman kita di atas, hal ini masuk akal. Matriks B memampatkan persegi dari gambar asli menjadi segmen garis, yang memiliki luas nol. Dan memang, dipampatkan ke dalam ruang dimensi yang lebih rendah adalah satu-satunya cara untuk memiliki luas nol setelah transformasi. Oleh karena itu, kita melihat hasil berikut ini benar: suatu matriks A dapat dibalik jika dan hanya jika determinannya tidak sama dengan nol.

As a final comment, imagine that we have any figure drawn on the plane. Thinking like computer scientists, we can decompose that figure into a collection of little squares so that the area of the figure is in essence just the number of squares in the decomposition. If we now transform that figure by a matrix, we send each of these squares to parallelograms, each one of which has area given by the determinant. We see that for any figure, the determinant gives the (signed) number that a matrix scales the area of any figure.

Sebagai komentar terakhir, bayangkan jika kita memiliki gambar apapun yang digambar pada bidang. Berpikir seperti seorang ilmuwan komputer, kita dapat membagi gambar tersebut menjadi kumpulan kotak kecil sehingga luas gambar pada dasarnya hanya merupakan jumlah kotak pada pembagian tersebut. Jika kita sekarang mentransformasikan gambar itu dengan sebuah matriks, kita mengirim setiap kotak ke segi empat sama kaki, di mana setiap satu memiliki luas yang diberikan oleh determinan. Kita melihat bahwa untuk gambar apapun, determinan memberikan (dalam bentuk bilangan berpola) faktor yang mengukur perubahan luas dari gambar oleh suatu matriks.

Computing determinants for larger matrices can be laborious, but the intuition is the same. The determinant remains the factor that $n\times n$ matrices scale -dimensional volumes.

Menghitung determinan untuk matriks yang lebih besar bisa sangat sulit, tetapi intuisinya tetap sama. Determinan tetap menjadi faktor yang mengukur perubahan volume n-dimensi dari matriks $n\times n$.

### 22.1.9. Tensors and Common Linear Algebra Operations¶

In Section 2.3 the concept of tensors was introduced. In this section, we will dive more deeply into tensor contractions (the tensor equivalent of matrix multiplication), and see how it can provide a unified view on a number of matrix and vector operations.

Pada bagian 2.3, diperkenalkan konsep tensor. Di bagian ini, kita akan lebih dalam mempelajari kontraksi tensor (yang setara dengan perkalian matriks), dan melihat bagaimana hal ini dapat memberikan pandangan yang terpadu pada sejumlah operasi matriks dan vektor.

With matrices and vectors we knew how to multiply them to transform data. We need to have a similar definition for tensors if they are to be useful to us. Think about matrix multiplication:

Dengan matriks dan vektor, kita tahu cara mengalikan mereka untuk mentransformasi data. Kita perlu memiliki definisi yang sama untuk tensor jika kita ingin menggunakannya dengan baik. Pikirkanlah tentang perkalian matriks:

$\mathbf{C} = \mathbf{A}\mathbf{B},$

or equivalently

$c_{i, j} = \sum_{k} a_{i, k}b_{k, j}.$

This pattern is one we can repeat for tensors. For tensors, there is no one case of what to sum over that can be universally chosen, so we need specify exactly which indices we want to sum over. For instance we could consider

Polanya ini dapat kita ulangi untuk tensor. Untuk tensor, tidak ada satu kasus pun dari apa yang harus dijumlahkan secara universal yang bisa dipilih, jadi kita perlu menentukan dengan tepat indeks mana yang ingin kita jumlahkan. Sebagai contoh, kita bisa mempertimbangkan:

$y_{il} = \sum_{jk} x_{ijkl}a_{jk}.$

Such a transformation is called a tensor contraction. It can represent a far more flexible family of transformations that matrix multiplication alone.

Transformasi seperti itu disebut kontraksi tensor. Kontraksi tensor dapat mewakili keluarga transformasi yang jauh lebih fleksibel daripada perkalian matriks.

As a often-used notational simplification, we can notice that the sum is over exactly those indices that occur more than once in the expression, thus people often work with Einstein notation, where the summation is implicitly taken over all repeated indices. This gives the compact expression:

Sebagai upaya penyederhanaan notasi yang sering digunakan, kita dapat memperhatikan bahwa penjumlahannya berlangsung tepat pada indeks-indeks yang muncul lebih dari satu kali dalam ekspresi tersebut. Oleh karena itu, orang-orang sering menggunakan notasi Einstein, di mana penjumlahan diambil secara implisit pada semua indeks yang berulang. Hal ini memberikan ekspresi yang padat:

$y_{il} = x_{ijkl}a_{jk}.$

### 22.1.9.1. Common Examples from Linear Algebra

Let’s see how many of the linear algebraic definitions we have seen before can be expressed in this compressed tensor notation:

Mari kita lihat berapa banyak definisi aljabar linier yang sudah kita pelajari sebelumnya yang bisa dinyatakan dalam notasi tensor yang terkompresi ini:

$(\mathbf{A}\mathbf{v})_i = \sum_j a_{ij}v_j$
$\|\mathbf{v}\|_2^{2} = \sum_i v_iv_i$
$(\mathbf{A}\mathbf{v})_i = \sum_j a_{ij}v_j$
$(\mathbf{A}\mathbf{B})_{ik} = \sum_j a_{ij}b_{jk}$
$\mathrm{tr}(\mathbf{A}) = \sum_i a_{ii}$

In this way, we can replace a myriad of specialized notations with short tensor expressions.

Dengan cara ini, kita bisa mengganti banyak notasi khusus dengan ekspresi tensor yang singkat.

### 22.1.9.2. Expressing in Code
Tensors may flexibly be operated on in code as well. As seen in Section 2.3, we can create tensors as is shown below.

Tensor dapat dioperasikan secara fleksibel dalam kode. Seperti yang terlihat pada bagian 2.3, kita dapat membuat tensor seperti yang ditunjukkan di bawah ini.

```python
# Define tensors
B = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
A = tf.constant([[1, 2], [3, 4]])
v = tf.constant([1, 2])

# Print out the shapes
A.shape, B.shape, v.shape
```
```python
(TensorShape([2, 2]), TensorShape([2, 2, 3]), TensorShape([2]))
```

Einstein summation has been implemented directly. The indices that occurs in the Einstein summation can be passed as a string, followed by the tensors that are being acted upon. For instance, to implement matrix multiplication, we can consider the Einstein summation seen above $\mathbf{A}\mathbf{v} = a_{ij}v_j$ and strip out the indices themselves to get the implementation:

Penjumlahan Einstein telah diimplementasikan secara langsung. Indeks yang muncul dalam penjumlahan Einstein dapat dilewatkan sebagai string, diikuti oleh tensor yang sedang dioperasikan. Sebagai contoh, untuk mengimplementasikan perkalian matriks, kita bisa mempertimbangkan penjumlahan Einstein yang terlihat di atas $\mathbf{A}\mathbf{v} = a_{ij}v_j$ dan menghilangkan indeks itu sendiri untuk mendapatkan implementasinya:

```python
# Reimplement matrix multiplication
tf.einsum("ij, j -> i", A, v), tf.matmul(A, tf.reshape(v, (2, 1)))
```
```python
(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 5, 11], dtype=int32)>,
 <tf.Tensor: shape=(2, 1), dtype=int32, numpy=
 array([[ 5],
        [11]], dtype=int32)>)
```
This is a highly flexible notation. For instance if we want to compute what would be traditionally written as

Ini adalah notasi yang sangat fleksibel. Sebagai contoh, jika kita ingin menghitung apa yang biasanya ditulis sebagai:

$c_{kl} = \sum_{ij} \mathbf{b}_{ijk}\mathbf{a}_{il}v_j.$

it can be implemented via Einstein summation as:

Hal itu dapat diimplementasikan melalui penjumlahan Einstein seperti ini:

```python
tf.einsum("ijk, il, j -> kl", B, A, v)
```
```python
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 90, 126],
       [102, 144],
       [114, 162]], dtype=int32)>
```

This notation is readable and efficient for humans, however bulky if for whatever reason we need to generate a tensor contraction programmatically. For this reason, einsum provides an alternative notation by providing integer indices for each tensor. For example, the same tensor contraction can also be written as:

Notasi ini mudah dibaca dan efisien untuk manusia, namun cukup berat jika untuk beberapa alasan kita perlu menghasilkan kontraksi tensor secara programatik. Oleh karena itu, einsum menyediakan notasi alternatif dengan menyediakan indeks bilangan bulat untuk setiap tensor. Misalnya, kontraksi tensor yang sama juga dapat ditulis sebagai berikut:

```python
# TensorFlow does not support this type of notation.
```

Either notation allows for concise and efficient representation of tensor contractions in code.

Kedua notasi tersebut memungkinkan representasi kontraksi tensor yang ringkas dan efisien dalam kode.

### 22.1.10. Summary
- Vectors can be interpreted geometrically as either points or directions in space.

- Dot products define the notion of angle to arbitrarily high-dimensional spaces.

- Hyperplanes are high-dimensional generalizations of lines and planes. They can be used to define decision planes that are often used as the last step in a classification task.

- Matrix multiplication can be geometrically interpreted as uniform distortions of the underlying coordinates. They represent a very restricted, but mathematically clean, way to transform vectors.

- Linear dependence is a way to tell when a collection of vectors are in a lower dimensional space than we would expect (say you have 3 vectors living in a 2-dimensional space). The rank of a matrix is the size of the largest subset of its columns that are linearly independent.

- When a matrix’s inverse is defined, matrix inversion allows us to find another matrix that undoes the action of the first. Matrix inversion is useful in theory, but requires care in practice owing to numerical instability.

- Determinants allow us to measure how much a matrix expands or contracts a space. A nonzero determinant implies an invertible (non-singular) matrix and a zero-valued determinant means that the matrix is non-invertible (singular).

- Tensor contractions and Einstein summation provide for a neat and clean notation for expressing many of the computations that are seen in machine learning.

Berikut ini adalah beberapa konsep matematika yang penting untuk dipahami dalam machine learning:

- Vektor dapat diinterpretasikan secara geometris sebagai titik atau arah dalam ruang.
- Produk dot (dot product) mendefinisikan konsep sudut pada ruang dengan dimensi yang tinggi.
- Hiperplane adalah generalisasi dimensi-tinggi dari garis dan bidang datar. Hiperplane dapat digunakan untuk mendefinisikan bidang keputusan yang seringkali digunakan sebagai langkah terakhir dalam tugas klasifikasi.
- Perkalian matriks dapat diinterpretasikan secara geometris sebagai distorsi seragam pada koordinat dasar. Perkalian matriks merepresentasikan cara yang sangat terbatas, tapi matematis yang bersih, untuk mentransformasi vektor.
- Ketergantungan linier adalah cara untuk mengetahui kapan suatu koleksi vektor berada di dalam ruang dimensi yang lebih rendah daripada yang diharapkan (misalnya, memiliki 3 vektor yang tinggal di dalam ruang 2 dimensi). Rank matriks adalah ukuran terbesar dari subset kolom matriks yang linear independen.
- Ketika sebuah matriks memiliki invers, invers matriks memungkinkan kita untuk menemukan matriks lain yang membatalkan aksi dari matriks pertama. Invers matriks sangat berguna dalam teori, tetapi memerlukan perhatian khusus dalam praktek karena ketidakstabilan numerik.
- Determinan memungkinkan kita untuk mengukur seberapa banyak suatu matriks memperluas atau mempersempit ruang
- Determinan yang tidak nol menunjukkan matriks yang dapat diinversi (non-singular) dan determinan bernilai nol berarti matriks tidak dapat diinversi (singular).
- Tensor kontraksi dan notasi Einstein summation menyediakan cara yang rapi dan bersih untuk mengekspresikan banyak komputasi yang ditemukan dalam machine learning.

### 22.1.11. Exercises
1. What is the angle between
```math
\begin{split}\vec v_1 = \begin{bmatrix}
1 \\ 0 \\ -1 \\ 2
\end{bmatrix}, \qquad \vec v_2 = \begin{bmatrix}
3 \\ 1 \\ 0 \\ 1
\end{bmatrix}?\end{split}
```
2. True or false: $\begin{bmatrix}1 & 2\\0&1\end{bmatrix}$ and $\begin{bmatrix}1 & -2\\0&1\end{bmatrix}$ are inverses of one another?

3. Suppose that we draw a shape in the plane with area $100\mathrm{m}^2$. What is the area after transforming the figure by the matrix
```math
\begin{split}\begin{bmatrix}
2 & 3\\
1 & 2
\end{bmatrix}.\end{split}
```

4. Which of the following sets of vectors are linearly independent?

- $\left\{\begin{pmatrix}1\\0\\-1\end{pmatrix}, \begin{pmatrix}2\\1\\-1\end{pmatrix}, \begin{pmatrix}3\\1\\1\end{pmatrix}\right\}$
- $\left\{\begin{pmatrix}3\\1\\1\end{pmatrix}, \begin{pmatrix}1\\1\\1\end{pmatrix}, \begin{pmatrix}0\\0\\0\end{pmatrix}\right\}$
- $\left\{\begin{pmatrix}1\\1\\0\end{pmatrix}, \begin{pmatrix}0\\1\\-1\end{pmatrix}, \begin{pmatrix}1\\0\\1\end{pmatrix}\right\}$

5. Suppose that you have a matrix written as $A = \begin{bmatrix}c\\d\end{bmatrix}\cdot\begin{bmatrix}a & b\end{bmatrix}$ for some choice of values a, b, c, and d. True or false: the determinant of such a matrix is always 0?

6. The vectors $e_1 = \begin{bmatrix}1\\0\end{bmatrix}$ and $e_2 = \begin{bmatrix}0\\1\end{bmatrix}$ are orthogonal. What is the condition on a matrix $A$ so that $Ae_1$ and $Ae_2$ are orthogonal?

7. How can you write $\mathrm{tr}(\mathbf{A}^4)$ in Einstein notation for an arbitrary matrix $A$?

### 22.2. Eigendecompositions

Eigenvalues are often one of the most useful notions we will encounter when studying linear algebra, however, as a beginner, it is easy to overlook their importance. Below, we introduce eigendecomposition and try to convey some sense of just why it is so important.

Suppose that we have a matrix A with the following entries:

Nilai Eigen (eigenvalues) seringkali menjadi konsep yang sangat berguna dalam mempelajari aljabar linear. Namun, bagi pemula, mudah untuk mengabaikan pentingnya konsep ini. Di bawah ini, kami akan memperkenalkan dekomposisi Eigen (eigendecomposition) dan mencoba memberikan pemahaman mengenai betapa pentingnya hal tersebut.

Bayangkan kita memiliki sebuah matriks A dengan entri-entri sebagai berikut:

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
2 & 0 \\
0 & -1
\end{bmatrix}.\end{split}
``` 

If we apply A to any vector $\mathbf{v} = [x, y]^\top$, we obtain a vector $\mathbf{A}\mathbf{v} = [2x, -y]^\top$. This has an intuitive interpretation: stretch the vector to be twice as wide in the x-direction, and then flip it in the y-direction.

However, there are some vectors for which something remains unchanged. Namely $[1, 0]^\top$ gets sent to $[2, 0]^\top$ and gets sent to $[0, -1]^\top$. These vectors are still in the same line, and the only modification is that the matrix stretches them by a factor of 2 and -1 respectively. We call such vectors eigenvectors and the factor they are stretched by eigenvalues.

In general, if we can find a number $\lambda$ and a vector v such that

Jika kita mengalikan matriks A dengan vektor $\mathbf{v} = [x, y]^\top$, kita akan mendapatkan vektor $\mathbf{A}\mathbf{v} = [2x, -y]^\top$. Ini memiliki interpretasi yang mudah dipahami: memperlebar vektor dua kali lebih lebar di arah x, lalu membalikannya di arah y.

Namun, ada beberapa vektor yang tidak mengalami perubahan. Yaitu $[1, 0]^\top$ yang dikirim menjadi $[2, 0]^\top$ dan dikirim menjadi $[0, -1]^\top$. Vektor-vektor ini masih berada pada garis yang sama, dan satu-satunya modifikasi adalah bahwa matriks meregangkannya dengan faktor 2 dan -1 masing-masing. Kita menyebut vektor-vektor seperti ini sebagai eigenvector dan faktor yang meregangkannya sebagai eigenvalue.

Secara umum, jika kita dapat menemukan sebuah bilangan $\lambda$ dan sebuah vektor v sehingga:

$\mathbf{A}\mathbf{v} = \lambda \mathbf{v}.$

We say that $\mathbf{v}$ is an eigenvector for A and $\lambda$ is an eigenvalue.

Kita menyebut $\mathbf{v}$ sebagai sebuah eigenvector (vektor eigen) untuk matriks A dan $\lambda$ sebagai eigenvalue (nilai eigen).

### 22.2.1. Finding Eigenvalues¶

Let’s figure out how to find them. By subtracting off the $\lambda \mathbf{v}$ from both sides, and then factoring out the vector, we see the above is equivalent to:

Mari kita mencari tahu bagaimana cara menemukannya. Dengan mengurangi $\lambda \mathbf{v}$ dari kedua sisi, lalu menarik keluar faktor vektor, kita dapat melihat bahwa persamaan di atas setara dengan:

```math
(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = 0.
```

For (22.2.3) to happen, we see that $(\mathbf{A} - \lambda \mathbf{I})$ must compress some direction down to zero, hence it is not invertible, and thus the determinant is zero. Thus, we can find the eigenvalues by finding for what $\lambda$ is $\det(\mathbf{A}-\lambda \mathbf{I}) = 0$. Once we find the eigenvalues, we can solve $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$ to find the associated eigenvector(s).

Untuk persamaan (22.2.3) terjadi, kita dapat melihat bahwa $(\mathbf{A} - \lambda \mathbf{I})$ harus meregangkan beberapa arah menjadi nol, sehingga matriks tersebut tidak dapat diinvers, dan dengan demikian determinannya adalah nol. Oleh karena itu, kita dapat menemukan eigenvalues dengan mencari untuk nilai $\lambda$ mana yang membuat $\det(\mathbf{A} - \lambda \mathbf{I}) = 0$. Setelah kita menemukan eigenvalues, kita dapat menyelesaikan $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$ untuk menemukan eigenvector yang terkait.

### 22.2.1.1. An Example

Let’s see this with a more challenging matrix

Mari kita lihat contoh ini dengan sebuah matriks yang lebih menantang.


```math
\begin{split}\mathbf{A} = \begin{bmatrix}
2 & 1\\
2 & 3
\end{bmatrix}.\end{split}
```

If we consider $\det(\mathbf{A}-\lambda \mathbf{I}) = 0$, we see this is equivalent to the polynomial equation $0 = (2-\lambda)(3-\lambda)-2 = (4-\lambda)(1-\lambda)$. Thus, two eigenvalues are 4 and 1
. To find the associated vectors, we then need to solve

Jika kita mempertimbangkan $\det(\mathbf{A}-\lambda \mathbf{I}) = 0$, kita dapat melihat bahwa ini setara dengan persamaan polinomial $0 = (2-\lambda)(3-\lambda)-2 = (4-\lambda)(1-\lambda)$. Oleh karena itu, dua eigenvalues adalah 4 dan 1. Untuk menemukan vektor-vektor yang terkait, maka kita perlu menyelesaikan:

```math
\begin{split}\begin{bmatrix}
2 & 1\\
2 & 3
\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} = \begin{bmatrix}x \\ y\end{bmatrix}  \; \text{and} \;
\begin{bmatrix}
2 & 1\\
2 & 3
\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}  = \begin{bmatrix}4x \\ 4y\end{bmatrix} .\end{split}
```

We can solve this with the vectors $[1, -1]^\top$ and $[1, 2]^\top$ respectively.

We can check this in code using the built-in numpy.linalg.eig routine.

Kita dapat menyelesaikan persamaan tersebut dengan vektor-vektor $[1, -1]^\top$ dan $[1, 2]^\top$ secara berturut-turut.

Kita dapat memeriksa ini dalam kode menggunakan rutin numpy.linalg.eig bawaan.

```python
%matplotlib inline
import tensorflow as tf
from IPython import display
from d2l import tensorflow as d2l

tf.linalg.eig(tf.constant([[2, 1], [2, 3]], dtype=tf.float64))
```
```python
(<tf.Tensor: shape=(2,), dtype=complex128, numpy=array([1.+0.j, 4.+0.j])>,
 <tf.Tensor: shape=(2, 2), dtype=complex128, numpy=
 array([[-0.70710678+0.j, -0.4472136 +0.j],
        [ 0.70710678+0.j, -0.89442719+0.j]])>)
```

Note that numpy normalizes the eigenvectors to be of length one, whereas we took ours to be of arbitrary length. Additionally, the choice of sign is arbitrary. However, the vectors computed are parallel to the ones we found by hand with the same eigenvalues.

Perlu diperhatikan bahwa numpy memperoleh eigenvector yang dinormalisasi sehingga panjangnya satu, sedangkan kita mengambil eigenvector dengan panjang yang sewenang-wenang. Selain itu, pilihan tanda juga sewenang-wenang. Namun, vektor-vektor yang dihitung oleh numpy sejajar dengan yang kita temukan secara manual dengan eigenvalues yang sama.

### 22.2.2. Decomposing Matrices

Let’s continue the previous example one step further. Let

Mari kita lanjutkan contoh sebelumnya satu langkah lebih jauh.Kita punya matriks

```math
\begin{split}\mathbf{W} = \begin{bmatrix}
1 & 1 \\
-1 & 2
\end{bmatrix},\end{split}
```

be the matrix where the columns are the eigenvectors of the matrix A . Let

di mana kolom-kolomnya adalah eigenvector dari matriks A. Selanjutnya, kita punya matriks

```math
\begin{split}\boldsymbol{\Sigma} = \begin{bmatrix}
1 & 0 \\
0 & 4
\end{bmatrix},\end{split}
```
 
be the matrix with the associated eigenvalues on the diagonal. Then the definition of eigenvalues and eigenvectors tells us that

di mana eigenvalues-nya terletak di diagonal. Maka definisi dari eigenvalues dan eigenvectors memberitahu kita bahwa

```math
\mathbf{A}\mathbf{W} =\mathbf{W} \boldsymbol{\Sigma} .
```

The matrix W is invertible, so we may multiply both sides by $W^{-1}$ on the right, we see that we may write

Matriks W dapat di-invert, sehingga kita dapat mengalikan kedua sisi dengan $W^{-1}$ di sisi kanan, sehingga kita dapat menuliskan

```math
\mathbf{A} = \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^{-1}.
```
In the next section we will see some nice consequences of this, but for now we need only know that such a decomposition will exist as long as we can find a full collection of linearly independent eigenvectors (so that W is invertible).

Pada bagian selanjutnya kita akan melihat beberapa konsekuensi yang bagus dari ini, tetapi untuk saat ini kita hanya perlu tahu bahwa dekomposisi seperti ini akan ada selama kita dapat menemukan kumpulan eigenvectors yang linearly independent penuh (sehingga W dapat di-invert).

### 22.2.3. Operations on Eigendecompositions

One nice thing about eigendecompositions (22.2.9) is that we can write many operations we usually encounter cleanly in terms of the eigendecomposition. As a first example, consider:

Salah satu keuntungan dari eigendecomposition (22.2.9) adalah bahwa kita dapat menuliskan banyak operasi yang sering kita temukan dengan jelas dalam bentuk eigendecomposition. Sebagai contoh pertama, pertimbangkan:

```math
\mathbf{A}^n = \overbrace{\mathbf{A}\cdots \mathbf{A}}^{\text{$n$ times}} = \overbrace{(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})\cdots(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})}^{\text{$n$ times}} =  \mathbf{W}\overbrace{\boldsymbol{\Sigma}\cdots\boldsymbol{\Sigma}}^{\text{$n$ times}}\mathbf{W}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^n \mathbf{W}^{-1}.
```

This tells us that for any positive power of a matrix, the eigendecomposition is obtained by just raising the eigenvalues to the same power. The same can be shown for negative powers, so if we want to invert a matrix we need only consider

Ini memberi tahu kita bahwa untuk setiap pangkat positif dari sebuah matriks, eigendecomposition dapat diperoleh dengan hanya mengangkat nilai eigen ke pangkat yang sama. Hal yang sama dapat ditunjukkan untuk pangkat negatif, sehingga jika kita ingin membalik sebuah matriks, kita hanya perlu mempertimbangkan

```math
\mathbf{A}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^{-1} \mathbf{W}^{-1},
```

or in other words, just invert each eigenvalue. This will work as long as each eigenvalue is non-zero, so we see that invertible is the same as having no zero eigenvalues.

Indeed, additional work can show that if $\lambda_1, \ldots, \lambda_n$ are the eigenvalues of a matrix, then the determinant of that matrix is

atau dengan kata lain, cukup membalikkan setiap eigenvalue. Ini akan berhasil selama setiap eigenvalue tidak sama dengan nol, sehingga kita melihat bahwa invertible sama dengan tidak memiliki eigenvalue nol.

Memang, kerja tambahan dapat menunjukkan bahwa jika $\lambda_1, \ldots, \lambda_n$ adalah eigenvalues dari sebuah matriks, maka determinan matriks tersebut adalah

```math
\det(\mathbf{A}) = \lambda_1 \cdots \lambda_n,
```

or the product of all the eigenvalues. This makes sense intuitively because whatever stretching W does, $W^{-1}$ undoes it, so in the end the only stretching that happens is by multiplication by the diagonal matrix $\boldsymbol{\Sigma}$, which stretches volumes by the product of the diagonal elements.

Finally, recall that the rank was the maximum number of linearly independent columns of your matrix. By examining the eigendecomposition closely, we can see that the rank is the same as the number of non-zero eigenvalues of A.

The examples could continue, but hopefully the point is clear: eigendecomposition can simplify many linear-algebraic computations and is a fundamental operation underlying many numerical algorithms and much of the analysis that we do in linear algebra.

atau hasil kali semua eigenvalues. Ini masuk akal secara intuitif karena apa pun yang dilakukan oleh W, $W^{-1}$ mengembalikannya, sehingga pada akhirnya satu-satunya perluasan yang terjadi adalah dengan perkalian oleh matriks diagonal $\boldsymbol{\Sigma}$, yang memperluas volume dengan hasil kali elemen diagonal.

Terakhir, ingatlah bahwa peringkat adalah jumlah maksimum kolom linearly independent dari matriks Anda. Dengan memeriksa eigendecomposition dengan cermat, kita dapat melihat bahwa peringkat sama dengan jumlah eigenvalues non-nol dari A.

Contoh-contoh ini bisa dilanjutkan, tetapi mudah-mudahan intinya sudah jelas: eigendecomposition dapat menyederhanakan banyak perhitungan aljabar linear dan merupakan operasi fundamental yang mendasari banyak algoritma numerik dan sebagian besar analisis yang kita lakukan dalam aljabar linear.

### 22.2.4. Eigendecompositions of Symmetric Matrices¶

It is not always possible to find enough linearly independent eigenvectors for the above process to work. For instance the matrix

Tidak selalu mungkin untuk menemukan cukup vektor eigen yang linear independen agar proses di atas berfungsi. Sebagai contoh, matriks

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix},\end{split}
```

has only a single eigenvector, namely $(1, 0)^\top$. To handle such matrices, we require more advanced techniques than we can cover (such as the Jordan Normal Form, or Singular Value Decomposition). We will often need to restrict our attention to those matrices where we can guarantee the existence of a full set of eigenvectors.

The most commonly encountered family are the symmetric matrices, which are those matrices where $\mathbf{A} = \mathbf{A}^\top$. In this case, we may take W to be an orthogonal matrix—a matrix whose columns are all length one vectors that are at right angles to one another, where $\mathbf{W}^\top = \mathbf{W}^{-1}$—and all the eigenvalues will be real. Thus, in this special case, we can write (22.2.9) as

hanya memiliki satu vektor eigen, yaitu $(1, 0)^\top$. Untuk menangani matriks seperti ini, kita memerlukan teknik yang lebih canggih daripada yang dapat kita bahas di sini (seperti Bentuk Normal Jordan, atau Dekomposisi Nilai Singular). Kita sering perlu membatasi perhatian kita pada matriks-matriks di mana kita dapat menjamin keberadaan satu set lengkap vektor eigen.

Kelompok matriks yang paling umum ditemui adalah matriks simetris, yang merupakan matriks di mana $\mathbf{A} = \mathbf{A}^\top$. Dalam hal ini, kita dapat mengambil W sebagai matriks ortogonal - matriks yang kolom-kolomnya semuanya adalah vektor panjang satu yang saling tegak lurus satu sama lain, di mana $\mathbf{W}^\top = \mathbf{W}^{-1}$ - dan semua eigenvalue akan menjadi bilangan real. Dalam kasus khusus ini, kita dapat menulis (22.2.9) sebagai

```math
\mathbf{A} = \mathbf{W}\boldsymbol{\Sigma}\mathbf{W}^\top .
```

### 22.2.5. Gershgorin Circle Theorem

Eigenvalues are often difficult to reason with intuitively. If presented an arbitrary matrix, there is little that can be said about what the eigenvalues are without computing them. There is, however, one theorem that can make it easy to approximate well if the largest values are on the diagonal.

Let $\mathbf{A} = (a_{ij})$ be any square matrix $(n\times n)$. We will define $r_i = \sum_{j \neq i} |a_{ij}|$ represent the disc in the complex plane with center $a_{ii}$ radius $r_i$. Then, every eigenvalue of A is contained in one of the $\mathcal{D}_i$.

This can be a bit to unpack, so let’s look at an example. Consider the matrix:

Eigenvalues seringkali sulit dipahami secara intuitif. Jika diberikan suatu matriks sembarang, tidak ada yang bisa dikatakan mengenai eigenvalues tanpa melakukan perhitungan terlebih dahulu. Namun, ada satu teorema yang dapat memudahkan untuk mengestimasi eigenvalues dengan baik jika nilai terbesar berada pada diagonal.

Misalkan $\mathbf{A} = (a_{ij})$ adalah suatu matriks persegi $(n\times n)$. Kita akan mendefinisikan $r_i = \sum_{j \neq i} |a_{ij}|$ yang merepresentasikan lingkaran di bidang kompleks dengan pusat $a_{ii}$ dan radius $r_i$. Maka, setiap eigenvalue dari $\mathbf{A}$ akan terdapat dalam salah satu dari $\mathcal{D}_i$.

Contoh matriks sebagai berikut:

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
1.0 & 0.1 & 0.1 & 0.1 \\
0.1 & 3.0 & 0.2 & 0.3 \\
0.1 & 0.2 & 5.0 & 0.5 \\
0.1 & 0.3 & 0.5 & 9.0
\end{bmatrix}.\end{split}
```

We have $r_1 = 0.3$, $r_2 = 0.6$, $r_3 = 0.8$ and r$_4 = 0.9$. The matrix is symmetric, so all eigenvalues are real. This means that all of our eigenvalues will be in one of the ranges of

Kita memiliki $r_1 = 0.3$, $r_2 = 0.6$, $r_3 = 0.8$ dan r$_4 = 0.9$. Matriks ini simetris, sehingga semua eigenvalues-nya adalah bilangan real. Artinya, semua eigenvalues kita akan berada di dalam salah satu dari rentang:

$[a_{11}-r_1, a_{11}+r_1] = [0.7, 1.3],$
$[a_{22}-r_2, a_{22}+r_2] = [2.4, 3.6],$
$[a_{33}-r_3, a_{33}+r_3] = [4.2, 5.8],$
$[a_{44}-r_4, a_{44}+r_4] = [8.1, 9.9].$

Performing the numerical computation shows that the eigenvalues are approximately 0.99, 2.97, 4.95, 9.08, all comfortably inside the ranges provided.

Melakukan perhitungan numerik menunjukkan bahwa eigenvalues-nya adalah sekitar 0.99, 2.97, 4.95, 9.08, yang semuanya berada di dalam rentang yang diberikan.

```python
A = tf.constant([[1.0, 0.1, 0.1, 0.1],
                [0.1, 3.0, 0.2, 0.3],
                [0.1, 0.2, 5.0, 0.5],
                [0.1, 0.3, 0.5, 9.0]])

v, _ = tf.linalg.eigh(A)
v
```
```python
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.99228525, 2.9734395 , 4.953943  , 9.080336  ], dtype=float32)>
```

In this way, eigenvalues can be approximated, and the approximations will be fairly accurate in the case that the diagonal is significantly larger than all the other elements.

It is a small thing, but with a complex and subtle topic like eigendecomposition, it is good to get any intuitive grasp we can.

Dengan cara ini, eigenvalues dapat diestimasi, dan estimasi yang diperoleh akan cukup akurat dalam kasus diagonal elemen matriks sangat jauh lebih besar dari elemen-elemen lainnya.

Ini adalah hal kecil, tetapi pada topik yang kompleks dan rumit seperti eigendecomposition, sangat bagus untuk memperoleh pemahaman yang intuitif.

### 22.2.6. A Useful Application: The Growth of Iterated Maps

Now that we understand what eigenvectors are in principle, let’s see how they can be used to provide a deep understanding of a problem central to neural network behavior: proper weight initialization.

### 22.2.6.1. Eigenvectors as Long Term Behavior

The full mathematical investigation of the initialization of deep neural networks is beyond the scope of the text, but we can see a toy version here to understand how eigenvalues can help us see how these models work. As we know, neural networks operate by interspersing layers of linear transformations with non-linear operations. For simplicity here, we will assume that there is no non-linearity, and that the transformation is a single repeated matrix operation A, so that the output of our model is

Pembahasan matematika lengkap tentang inisialisasi jaringan saraf dalam sangat kompleks, namun di sini kita akan membahas versi sederhana agar dapat memahami bagaimana eigenvalue dapat membantu kita dalam memahami cara kerja model-model ini. Seperti yang kita ketahui, jaringan saraf bekerja dengan menggabungkan lapisan-lapisan transformasi linear dengan operasi non-linear. Untuk kemudahan di sini, kita akan mengasumsikan bahwa tidak ada non-linearitas dan bahwa transformasi terdiri dari satu operasi matriks yang diulang, yaitu:

```math
\mathbf{v}_{out} = \mathbf{A}\cdot \mathbf{A}\cdots \mathbf{A} \mathbf{v}_{in} = \mathbf{A}^N \mathbf{v}_{in}.
```

When these models are initialized, A is taken to be a random matrix with Gaussian entries, so let’s make one of those. To be concrete, we start with a mean zero, variance one Gaussian distributed $5 \times 5$ matrix.

Saat model-model ini diinisialisasi, matriks A diambil sebagai matriks acak dengan entri Gaussian, jadi mari kita buat satu matriks seperti itu. Secara khusus, kita akan memulai dengan matriks Gaussian dengan dimensi $5 \times 5$, dengan rata-rata nol dan variansi satu.

```python
k = 5
A = tf.random.normal((k, k), dtype=tf.float64)
A
```
```python
<tf.Tensor: shape=(5, 5), dtype=float64, numpy=
array([[ 0.63083335, -0.01714095,  0.66207166,  0.93973443,  1.93534878],
       [-1.52735455,  1.74458178, -0.76342079,  0.85655309, -0.67900872],
       [ 0.71279994, -1.70399866,  0.54162547,  0.22933718, -0.04556216],
       [ 0.10306052,  0.80454179, -0.78732833,  0.30007514,  1.15642235],
       [-2.32285654, -1.9747878 , -0.52170076,  0.66721778, -0.18499969]])>
```

### 22.2.6.2. Behavior on Random Data

For simplicity in our toy model, we will assume that the data vector we feed in $\mathbf{v}_{in}$ is a random five dimensional Gaussian vector. Let’s think about what we want to have happen. For context, lets think of a generic ML problem, where we are trying to turn input data, like an image, into a prediction, like the probability the image is a picture of a cat. If repeated application of A stretches a random vector out to be very long, then small changes in input will be amplified into large changes in output—tiny modifications of the input image would lead to vastly different predictions. This does not seem right!

On the flip side, if A shrinks random vectors to be shorter, then after running through many layers, the vector will essentially shrink to nothing, and the output will not depend on the input. This is also clearly not right either!

We need to walk the narrow line between growth and decay to make sure that our output changes depending on our input, but not much!

Let’s see what happens when we repeatedly multiply our matrix A against a random input vector, and keep track of the norm.

Dalam model mainan kita yang sederhana ini, kita akan mengasumsikan bahwa vektor data yang kita masukkan $\mathbf{v}_{in}$ adalah vektor Gaussian lima dimensi acak. Mari kita pikirkan tentang apa yang ingin kita lakukan. Untuk konteksnya, mari kita bayangkan masalah pembelajaran mesin umum di mana kita mencoba mengubah data masukan seperti gambar menjadi prediksi, seperti probabilitas gambar tersebut adalah gambar kucing. Jika aplikasi berulang dari A meregangkan vektor acak menjadi sangat panjang, maka perubahan kecil pada masukan akan diperbesar menjadi perubahan besar pada keluaran - modifikasi kecil pada gambar masukan akan mengarah pada prediksi yang sangat berbeda. Ini tidak terlihat benar!

Di sisi lain, jika A menyusutkan vektor acak menjadi lebih pendek, maka setelah dijalankan melalui banyak lapisan, vektor akan menyusut menjadi tidak ada, dan keluaran tidak akan tergantung pada masukan. Ini juga jelas tidak benar!

Kita perlu menjaga keseimbangan antara pertumbuhan dan kemunduran untuk memastikan bahwa output kita berubah tergantung pada masukan kita, tapi tidak terlalu banyak!

Mari kita lihat apa yang terjadi ketika kita mengalikan berulang-ulang matriks A dengan vektor masukan acak, dan melacak normanya.

```python
# Calculate the sequence of norms after repeatedly applying `A`
v_in = tf.random.normal((k, 1), dtype=tf.float64)

norm_list = [tf.norm(v_in).numpy()]
for i in range(1, 100):
    v_in = tf.matmul(A, v_in)
    norm_list.append(tf.norm(v_in).numpy())

d2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')
```

![](https://d2l.ai/_images/output_eigendecomposition_ee2e00_45_0.svg)

The norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a pattern.

Norma sedang tumbuh secara tidak terkendali! Memang, jika kita mengambil daftar kuartil, kita akan melihat pola.

```python
# Compute the scaling factor of the norms
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
```
![](https://d2l.ai/_images/output_eigendecomposition_ee2e00_57_0.svg)

If we look at the last portion of the above computation, we see that the random vector is stretched by a factor of 1.974459321485[...], where the portion at the end shifts a little, but the stretching factor is stable.

Jika kita melihat pada bagian terakhir dari perhitungan di atas, kita melihat bahwa vektor acak diregangkan dengan faktor 1,974459321485[...], di mana bagian di akhir sedikit bergeser, tetapi faktor penggerekannya stabil.

### 22.2.6.3. Relating Back to Eigenvectors

We have seen that eigenvectors and eigenvalues correspond to the amount something is stretched, but that was for specific vectors, and specific stretches. Let’s take a look at what they are for A. A bit of a caveat here: it turns out that to see them all, we will need to go to complex numbers. You can think of these as stretches and rotations. By taking the norm of the complex number (square root of the sums of squares of real and imaginary parts) we can measure that stretching factor. Let’s also sort them.

```python
# Compute the eigenvalues
eigs = tf.linalg.eigh(A)[0].numpy().tolist()
norm_eigs = [tf.abs(tf.constant(x, dtype=tf.float64)) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
```
```python
norms of eigenvalues: [<tf.Tensor: shape=(), dtype=float64, numpy=0.5257789021389455>, <tf.Tensor: shape=(), dtype=float64, numpy=0.6516916425639406>, <tf.Tensor: shape=(), dtype=float64, numpy=2.7424358325313816>, <tf.Tensor: shape=(), dtype=float64, numpy=3.6406376885216702>, <tf.Tensor: shape=(), dtype=float64, numpy=4.0562306528556284>]
```

### 22.2.6.4. An Observation
We see something a bit unexpected happening here: that number we identified before for the long term stretching of our matrix A applied to a random vector is exactly (accurate to thirteen decimal places!) the largest eigenvalue of A. This is clearly not a coincidence!

But, if we now think about what is happening geometrically, this starts to make sense. Consider a random vector. This random vector points a little in every direction, so in particular, it points at least a little bit in the same direction as the eigenvector of A associated with the largest eigenvalue. This is so important that it is called the principle eigenvalue and principle eigenvector. After applying A , our random vector gets stretched in every possible direction, as is associated with every possible eigenvector, but it is stretched most of all in the direction associated with this principle eigenvector. What this means is that after apply in A, our random vector is longer, and points in a direction closer to being aligned with the principle eigenvector. After applying the matrix many times, the alignment with the principle eigenvector becomes closer and closer until, for all practical purposes, our random vector has been transformed into the principle eigenvector! Indeed this algorithm is the basis for what is known as the power iteration for finding the largest eigenvalue and eigenvector of a matrix. For details see, for example, (Van Loan and Golub, 1983).

Kita telah melihat bahwa vektor eigen dan nilai eigen berkaitan dengan seberapa besar suatu objek diregangkan, namun itu hanya untuk vektor dan regangan tertentu saja. Mari kita lihat nilai eigen dan vektor eigen dari matriks A. Namun, satu catatan penting: untuk melihat semuanya, kita harus menggunakan bilangan kompleks. Kita dapat memikirkannya sebagai perluasan dari konsep peregangan dan rotasi. Dengan mengambil norma bilangan kompleks (akar kuadrat dari jumlah kuadrat dari bagian riil dan imajiner) kita dapat mengukur faktor peregangan tersebut. Mari kita urutkan dari yang terbesar.

Kita melihat sesuatu yang sedikit tak terduga terjadi di sini: angka yang kita identifikasi sebelumnya untuk peregangan jangka panjang matriks A yang diterapkan pada vektor acak adalah nilai eigen terbesar dari A. Ini jelas bukan kebetulan!

Namun, jika kita memikirkan apa yang terjadi secara geometris, ini mulai masuk akal. Pertimbangkan vektor acak. Vektor acak ini menunjuk sedikit ke semua arah, jadi terutama, ia menunjuk setidaknya sedikit ke arah yang sama dengan vektor eigen dari A yang terkait dengan nilai eigen terbesar. Ini sangat penting sehingga disebut nilai eigen utama dan vektor eigen utama. Setelah menerapkan A, vektor acak kita meregang ke setiap arah yang mungkin, sesuai dengan setiap vektor eigen yang mungkin, tetapi meregang paling banyak ke arah yang terkait dengan vektor eigen utama ini. Artinya, setelah menerapkan A, vektor acak kita lebih panjang, dan menunjuk ke arah yang lebih dekat dengan sejajar dengan vektor eigen utama. Setelah menerapkan matriks berkali-kali, penyejajaran dengan vektor eigen utama semakin dekat sampai, untuk semua tujuan praktis, vektor acak kita telah diubah menjadi vektor eigen utama! Memang, algoritma ini adalah dasar dari apa yang dikenal sebagai iterasi daya untuk menemukan nilai eigen dan vektor eigen terbesar dari sebuah matriks. Untuk rincian, lihat, misalnya, (Van Loan dan Golub, 1983).

### 22.2.6.5. Fixing the Normalization
Now, from above discussions, we concluded that we do not want a random vector to be stretched or squished at all, we would like random vectors to stay about the same size throughout the entire process. To do so, we now rescale our matrix by this principle eigenvalue so that the largest eigenvalue is instead now just one. Let’s see what happens in this case.

Dari diskusi sebelumnya, kita menyimpulkan bahwa kita tidak ingin vektor acak diregangkan atau dicubit sama sekali, kita ingin vektor acak tetap sekitar ukuran yang sama sepanjang seluruh proses. Untuk melakukannya, sekarang kita akan menskalakan kembali matriks kita dengan eigenvalue prinsipal sehingga eigenvalue terbesar sekarang menjadi satu. Mari kita lihat apa yang terjadi dalam kasus ini.

```python
# Rescale the matrix `A`
A /= norm_eigs[-1]

# Do the same experiment again
v_in = tf.random.normal((k, 1), dtype=tf.float64)

norm_list = [tf.norm(v_in).numpy()]
for i in range(1, 100):
    v_in = tf.matmul(A, v_in)
    norm_list.append(tf.norm(v_in).numpy())

d2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')
```
![](https://d2l.ai/_images/output_eigendecomposition_ee2e00_81_0.svg)

We can also plot the ratio between consecutive norms as before and see that indeed it stabilizes.

Kita juga dapat menampilkan grafik rasio antara norma-norma berturut-turut seperti sebelumnya dan melihat bahwa memang norma-norma tersebut stabil.

```pyhton
# Also plot the ratio
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
```
![](https://d2l.ai/_images/output_eigendecomposition_ee2e00_93_0.svg)

### 22.2.7. Discussion

We now see exactly what we hoped for! After normalizing the matrices by the principal eigenvalue, we see that the random data does not explode as before, but rather eventually equilibrates to a specific value. It would be nice to be able to do these things from first principles, and it turns out that if we look deeply at the mathematics of it, we can see that the largest eigenvalue of a large random matrix with independent mean zero, variance one Gaussian entries is on average about $\sqrt{n}$, or in our case $\sqrt{5} \approx 2.2$, due to a fascinating fact known as the circular law (Ginibre, 1965). The relationship between the eigenvalues (and a related object called singular values) of random matrices has been shown to have deep connections to proper initialization of neural networks as was discussed in Pennington et al. (2017) and subsequent works.

Sekarang, kita melihat apa yang kita harapkan! Setelah mengnormalisasi matriks dengan eigenvalue utama, kita melihat bahwa data acak tidak meledak seperti sebelumnya, melainkan akhirnya menyeimbangkan pada nilai tertentu. Sangat baik jika kita dapat melakukan hal-hal ini dari prinsip-prinsip awal, dan ternyata jika kita melihat secara mendalam pada matematika di baliknya, kita dapat melihat bahwa eigenvalue terbesar dari matriks acak besar dengan entri Gaussian mean nol independen, variansi satu pada rata-rata sekitar $\sqrt{n}$, atau dalam kasus kita sekitar $\sqrt{5} \approx 2.2$, karena sebuah fakta menarik yang dikenal sebagai hukum lingkaran (Ginibre, 1965). Hubungan antara eigenvalue (dan objek terkait yang disebut singular value) dari matriks acak telah ditunjukkan memiliki koneksi yang dalam dengan inisialisasi yang tepat dari jaringan saraf seperti yang dibahas dalam Pennington et al. (2017) dan karya-karya berikutnya.

### 22.2.8. Summary
- Eigenvectors are vectors which are stretched by a matrix without changing direction.

- Eigenvalues are the amount that the eigenvectors are stretched by the application of the matrix.

- The eigendecomposition of a matrix can allow for many operations to be reduced to operations on the eigenvalues.

- The Gershgorin Circle Theorem can provide approximate values for the eigenvalues of a matrix.

- The behavior of iterated matrix powers depends primarily on the size of the largest eigenvalue. This understanding has many applications in the theory of neural network initialization.

Eigenvector adalah vektor yang ditarik (stretched) oleh matriks tanpa mengubah arah.

Eigenvalue adalah jumlah yang digunakan untuk merepresentasikan seberapa jauh vektor eigen tersebut ditarik oleh matriks.

Dekomposisi eigen dari sebuah matriks dapat memungkinkan banyak operasi untuk direduksi menjadi operasi pada eigenvalue.

Teorema Gershgorin Circle dapat memberikan nilai perkiraan untuk eigenvalue dari sebuah matriks.

Perilaku dari iterasi kuasa matriks tergantung pada ukuran eigenvalue terbesar. Pemahaman ini memiliki banyak aplikasi dalam teori inisialisasi jaringan syaraf (neural network).

### 22.2.9. Exercises

1. What are the eigenvalues and eigenvectors of

    Eigenvalue dan eigenvector dari matriks A adalah:

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}?\end{split}
```
2. What are the eigenvalues and eigenvectors of the following matrix, and what is strange about this example compared to the previous one?

    Apa saja eigenvalues dan eigenvectors dari matriks berikut, dan apa yang aneh dari contoh ini dibandingkan dengan yang sebelumnya?

```math
\begin{split}\mathbf{A} = \begin{bmatrix}
2 & 1 \\
0 & 2
\end{bmatrix}.\end{split}
```

3. Without computing the eigenvalues, is it possible that the smallest eigenvalue of the following matrix is less that 0.5 ? Note: this problem can be done in your head.

    Apakah mungkin nilai eigenvalue terkecil dari matriks berikut kurang dari 0,5 tanpa menghitungnya? Catatan: masalah ini dapat diselesaikan dalam pikiran Anda.


```math
\begin{split}\mathbf{A} = \begin{bmatrix}
3.0 & 0.1 & 0.3 & 1.0 \\
0.1 & 1.0 & 0.1 & 0.2 \\
0.3 & 0.1 & 5.0 & 0.0 \\
1.0 & 0.2 & 0.0 & 1.8
\end{bmatrix}.\end{split}
```